# Enhancing HOSER Trajectory Generation with Anomaly Detection from LM-TAD

## Executive Summary

The challenge of generating highly realistic and plausible taxi trajectories requires a sophisticated model that not only understands the physical topology of a city but also the behavioral nuances that define "normal" movement. This analysis explores a novel and highly promising approach: using a trajectory anomaly detection model, such as LM-TAD, as a real-time "co-pilot" or external critic to guide a generative model like HOSER.

HOSER's existing architecture is designed to produce a trajectory that maximizes its own internal probability function. However, a model like LM-TAD is specifically trained to identify what constitutes a statistically "normal" or "anomalous" trajectory, even at a fine-grained, point-by-point level. By integrating these two models, HOSER's generation process can be guided by LM-TAD's expertise, allowing it to actively avoid improbable or anomalous paths in real time.

This report outlines a technical blueprint for this integration, demonstrating how LM-TAD's perplexity and surprisal rate metrics can serve as a powerful, quantitative feedback loop to improve HOSER's trajectory generation. The proposed methodology is expected to significantly enhance the fidelity of generated trajectories, resulting in better scores across all key evaluation metrics, including JSD, DTW, and EDR.

## 1. Deconstructing the Two Foundational Models

### 1.1. HOSER: A Generative Trajectory Framework

The Holistic SEmantic Representation (HOSER) framework is a generative model designed to synthesize high-fidelity human mobility trajectories. Its core objective is to learn a policy $P(r_{i+1}|x_{1:i}, r_{dest})$, which represents the probability of moving to the next road segment $r_{i+1}$ given a partial trajectory $x_{1:i}$ and a destination $r_{dest}$. The final trajectory is generated by a search-based method that aims to find the path with the maximum cumulative probability.

HOSER's architecture comprises three key modules:

- **Road Network Encoder**: Captures the city's road network topology at both road and zone levels using Graph Attention Networks (GATv2) and Graph Convolutional Networks (GCN).
- **Multi-Granularity Trajectory Encoder**: Uses a Decoder-Only Transformer to model spatio-temporal relationships within the partial trajectory, incorporating features like distances and time intervals between points.
- **Destination-Oriented Navigator**: The final decision-making module that predicts the next road segment by fusing the partial trajectory's semantics with destination guidance, including geometric characteristics like distance and angle to the destination.

### 1.2. LM-TAD: An Anomaly Detection Model

The Language Model for Trajectory Anomaly Detection (LM-TAD) is a novel approach that re-frames trajectory data as a language processing task. It uses an autoregressive causal-attention model, similar to those found in Large Language Models (LLMs), to learn the probability distribution of a trajectory by predicting the next location in a sequence given its history.

LM-TAD's core strengths, which are highly complementary to HOSER's generative function, include:

- **User-Contextual Anomaly Detection**: The model can be trained to account for individual behavior patterns by incorporating user-specific tokens (USER_ID), allowing it to flag what is anomalous for one user but normal for another.
- **Point-by-Point Anomaly Localization**: Unlike previous methods that assign an anomaly score to an entire trajectory, LM-TAD can pinpoint the exact location of an anomaly within a trajectory using a metric called the surprisal rate. A high surprisal rate for a particular location indicates a low-probability, and therefore potentially anomalous, event.
- **Real-time Efficiency**: The model is well-suited for online anomaly detection because it can cache key-value states of its attention mechanism, significantly reducing computational latency and enabling real-time analysis as a trajectory is being generated.
- **Perplexity as a Trajectory-Level Metric**: LM-TAD uses perplexity, a common metric in natural language processing, to measure the overall uncertainty of the model's predictions for an entire trajectory. A high perplexity score indicates an anomalous or low-probability trajectory, making it an excellent metric for identifying outliers in a dataset.

## 2. A Blueprint for Integration: LM-TAD as a Real-Time Critic

The most impactful and feasible method for enhancing HOSER is to leverage a pre-trained LM-TAD model as an external critic, guiding HOSER's generative process toward more realistic and less anomalous outcomes. This approach moves beyond simple generative modeling by introducing a second, specialized model that is an expert in identifying "plausibility." This is a concept similar to multi-critic architectures used in reinforcement learning.

The proposed integration would work as follows:

### Pre-train LM-TAD
A separate LM-TAD model is first trained on the real-world taxi trajectory dataset. Its objective is to become an expert at identifying normal versus anomalous trajectories, learning the intricate sequential and user-specific patterns that define typical taxi behavior.

### Augmenting HOSER's Navigator
The core enhancement is implemented within HOSER's Destination-Oriented Navigator, the module that iteratively predicts the next road segment. The current Navigator's logit calculation (Eq. 12 in the HOSER paper) primarily uses geometric features like distance and angle to the destination. The proposed change would be to introduce a new, LM-TAD-derived metric. At each step $i$, before selecting the next road segment $r_c$ from the set of reachable roads $R(r_i)$, HOSER would query the pre-trained LM-TAD model for the surprisal rate of $r_c$ given the current partial trajectory $x_{1:i}$.

### Integrating the Surprisal Rate
This surprisal rate value would then be used to modify the Navigator's logit calculation. A candidate road segment with an unusually high surprisal rate from LM-TAD—indicating that it is an improbable choice given the context—would be penalized. This effectively means that HOSER's Navigator would learn to not only select the most geometrically and topologically sound path but also the most "semantically normal" one, as judged by the LM-TAD critic. The computationally efficient caching mechanism of LM-TAD would allow this to happen in real time without significant latency.

### A Combined Loss Function
During HOSER's training, the model's objective would be a composite loss function. In addition to the negative log-likelihood for road segment prediction and the mean absolute error for time prediction, a new term would be added: a penalty proportional to the surprisal rate of the selected road segment. This would directly incentivize HOSER to generate trajectories that are not just self-consistent but are also considered "normal" by an external, expert anomaly detection model.

## 3. Implementation Plan and Core Mathematical Formalism

### 3.1 Concrete Implementation Plan

The integration of LM-TAD as a real-time critic for HOSER will be implemented in three distinct phases:

**Phase 1: Pre-training the Critic**
The LM-TAD model, a causal-attention based language model, will be trained independently on the target trajectory dataset (e.g., Beijing taxi data). The goal of this phase is to train LM-TAD to become an expert in the normal probability distribution of trajectories. The model learns to predict the next location in a sequence given its history, and this training results in a model that can quantify the surprisal rate—a measure of how unexpected a given road segment is in a specific context. The output of this phase is a frozen, pre-trained LM-TAD model that will serve as the external critic.

**Phase 2: Augmenting HOSER's Training Loop**
The training loop of the HOSER model will be modified to include a new step. During each iteration, for every new road segment $r_{i+1}$ that HOSER's Destination-Oriented Navigator predicts, the pre-trained LM-TAD model will be queried. This query will take the partial trajectory $x_{1:i}$ as input and return the surprisal rate of $r_{i+1}$. LM-TAD is well-suited for this real-time, online process due to its efficient caching mechanism, which prevents the need for recomputing the entire history at each step.

**Phase 3: Optimizing with a Combined Loss Function**
The final and most critical step is to replace HOSER's original loss function with a new, composite loss function that incorporates the surprisal rate from LM-TAD. This new loss function will penalize HOSER for generating trajectories that are considered improbable by the critic. The model will then be trained to minimize this combined loss, which forces it to learn a generative policy that not only produces topologically sound paths but also avoids behaviorally anomalous ones.

### 3.2 Core Mathematical Formalism

#### LM-TAD's Core Metrics

The LM-TAD model's value lies in its ability to quantify the plausibility of a trajectory or a specific point within it. It achieves this using two key metrics:

**Perplexity (PPL)**: A trajectory-level metric that measures how well the model predicts the sequence of locations. Perplexity can be thought of as the weighted average number of choices the model has for each location. A lower perplexity score indicates that the model is more confident in its predictions and the trajectory is therefore more "normal". The perplexity of a trajectory $T$ with $t$ locations is calculated as:

$$PPL(T) = \exp\left\{-\frac{1}{t}\sum_{i=1}^t \log P(l_i | l_{<i})\right\}$$

**Surprisal Rate**: A point-by-point metric that quantifies the unexpectedness of a single location (road segment) given the preceding context. It is mathematically defined as the negative log-likelihood of the next location:

$$\text{Surprisal}(l_{i+1}) = -\log P(l_{i+1} | l_{<i+1})$$

A high surprisal rate for a particular road segment indicates that it is a low-probability, and therefore potentially anomalous, choice. This metric is ideal for real-time guidance within HOSER's generative process.

#### HOSER's Original and New Combined Loss

HOSER's original training objective is to minimize a composite loss function $\mathcal{L}$, which combines the negative log-likelihood for the next road segment prediction and the mean absolute error for the time interval prediction. This is represented as:

$$\mathcal{L} = \frac{1}{n-1}\sum_{i=1}^{n-1} [\mathcal{L}_{road} + \mathcal{L}_{time}]$$

The new, enhanced loss function, $\mathcal{L}_{total}$, will add a new term, $\mathcal{L}_{critic}$, to this original loss. This critic loss is a weighted sum of the LM-TAD surprisal rates for all road segments in the generated trajectory. The new, combined loss is:

$$\mathcal{L}_{total} = \mathcal{L} + \mathcal{L}_{critic}$$

Where the new critic term is:

$$\mathcal{L}_{critic} = \lambda \cdot \sum_{i=1}^{n-1} \text{Surprisal}_{LM-TAD}(r_{i+1})$$

Here, $\lambda$ is a hyperparameter that controls the influence of the LM-TAD critic. By minimizing this combined loss, HOSER is incentivized to generate trajectories that are not only geometrically and topologically sound but also behaviorally plausible and low-surprisal as defined by the external LM-TAD model.

## 4. GPU Memory Footprint and Training Practicalities

Modern GPUs like the RTX 2080 Ti provide **≈ 11 GB** of usable VRAM.  Will HOSER *plus* an LM-TAD critic fit? Yes—but only with the right precision and batch sizes.  The table below summarises the static (parameter) memory in FP16:

| Component | Params | FP16 size |
|-----------|--------|-----------|
| HOSER road–, type– & zone-embeddings | ≈ 10 M | ~20 MB |
| HOSER GAT/GCN + trajectory decoder | ≈ 2 M | ~4 MB |
| **HOSER total** | **≈ 12 M** | **~24 MB** |
| LM-TAD (8-layer, 768-d) | ≈ 65 M | ~125 MB |
| **Both together** | **77 M** | **~150 MB** |

Static weights therefore occupy < 200 MB—trivial compared with 11 GB.  The real memory pressure comes from **training-time tensors**:

1. **Optimizer states** (AdamW keeps 2 FP32 copies) → +2× param size.
2. **Activations** retained for back-prop.  For LM-TAD they scale with *batch × seq × layers*; for HOSER the quadratic `trace_distance_mat` & `trace_time_interval_mat` dominate.
3. **CUDA allocator arena** (~1 GB) and cuDNN workspace.

### Why default batch sizes OOM
* **HOSER** default `batch_size = 64`, `max_len = 200` → two `(B,L,L)` FP16 matrices = 64×200×200×2×2 B ≈ 200 MB *each*, held twice (fwd+grad).  Plus candidate tensors `(B,L,C)`.
* **LM-TAD** default `batch_size = 64`, `block_size = 128` → hidden states ≈ 1 GB; gradients double that.

Together they push peak memory > 12 GB, hence you had to slash the batches.

### Proven remedies
* **Mixed precision** (`torch.cuda.amp` – already enabled).
* **Gradient checkpointing** on LM-TAD and HOSER’s decoder → 40–60 % activation cut.
* **8-bit Adam** (bits-and-bytes) or Adafactor → –75 % optimizer memory.
* **Gradient accumulation**: keep small `batch_size_gpu`, accumulate to restore the effective batch.
* **Shorter windows**: LM-TAD critic rarely needs > 64 tokens; HOSER distance/time matrices can be bucketed or cropped.
* **Sequential swaps** at inference: move one model to CPU while the other runs (not usually required but always safe).

With these levers a single 2080 Ti can (and does) train HOSER at `batch_size = 16–32` and LM-TAD at `batch_size = 8–16`, while inference-time co-execution is < 2 GB.

## 5. Anticipated Impact on Evaluation Metrics

This integration is expected to have a profound impact on HOSER's performance, leading to significant improvements across all its evaluation metrics.

### Enhanced Local Fidelity (Lower DTW and EDR)
By using LM-TAD's surprisal rate to guide the Navigator's micro-level decisions, HOSER will be able to avoid subtle, improbable choices that might deviate from real-world routes. This will lead to a better point-for-point match with real trajectories that share the same origin-destination pair, resulting in lower DTW and EDR scores.

### Improved Global Distributions (Lower JSD)
The overarching goal of this integration is to make generated trajectories more "normal" and plausible. By making better local decisions, the aggregated statistical properties of the entire synthetic dataset will also improve. For instance, the distributions of distance, radius of gyration, and duration will more accurately reflect the multimodal nature of real taxi trips. This will result in a lower Jensen-Shannon divergence (JSD) score, confirming that the synthetic data is statistically indistinguishable from real data and therefore more useful for downstream applications.

## 6. Review of Relevant Research and Code

LM-TAD provides a strong precedent for using language models for trajectory analysis. The authors have made the code to reproduce their experiments publicly available at [https://github.com/jonathankabala/LMTAD](https://github.com/jonathankabala/LMTAD). The HOSER framework's code is also open-source at [https://github.com/caoji2001/HOSER](https://github.com/caoji2001/HOSER). This makes the proposed integration a technically feasible and a practical research direction.

The LM-TAD paper's findings, particularly on the Porto taxi dataset, show that a model can effectively identify outliers in GPS-based trajectory data and that its performance is competitive with other state-of-the-art methods. This validates the use of LM-TAD's metrics for quality control in a generative model.

While this specific integration has not been explored in the provided literature, the general concept of using multiple "critics" or reward functions to guide a generative process is a well-established practice in reinforcement learning research. This demonstrates that the proposed approach, while novel for trajectory generation, is grounded in sound machine learning principles.

## 7. Conclusion

By shifting the focus from simply generating trajectories to generating trajectories that are both topologically sound and behaviorally plausible, the LM-TAD-enhanced HOSER model represents a significant step forward. The integration of LM-TAD as a real-time critic provides HOSER with a crucial layer of semantic and contextual understanding that is essential for modeling the strategic, professional behaviors of taxi drivers. 

The ability to use LM-TAD's perplexity and surprisal rate to guide HOSER's pathing decisions will not only lead to more accurate and realistic generated data but will also provide a new, quantitative method for evaluating the plausibility of synthetic trajectories. This approach offers a clear and feasible path to building a new generation of generative models that are more robust, context-aware, and ultimately more useful for real-world applications in urban mobility.

## 8. Concrete Implementation Plan: LM‑TAD Critic in nx_astar (gene.py)

This section details a practical, low-risk path to integrate LM‑TAD as a real-time critic into the trajectory generation pipeline, specifically targeting the `nx_astar` mode used in `gene.py`. The plan avoids invasive changes, keeps HOSER’s timestamping intact, and adds an opt‑in, history‑aware A* that penalizes high‑surprisal moves.

### 8.1 Scope and Key Idea

- Integrate LM‑TAD as a candidate scorer during A* neighbor expansion. For each hop, compute a step cost that combines edge length with an LM‑TAD surprisal penalty of the candidate next road given the history.
- Keep the existing `nx_astar_search` method unchanged; add a new `astar_with_critic` method and a CLI flag to enable it.
- Continue to use HOSER only for timestamping along the final path (unchanged logic).

### 8.2 Files to Add / Modify

- Add: `critics/lmtad_wrapper.py` — a lightweight LM‑TAD scorer with KV caching.
- Modify: `gene.py`
  - Add CLI flags: `--use_critic`, `--critic_repo`, `--critic_ckpt`, `--critic_lambda`, `--critic_block_size`, `--critic_gpu`.
  - Add `Searcher.astar_with_critic(...)` that performs history‑aware A*.
  - In `__main__`, if `--nx_astar` and `--use_critic` then call `astar_with_critic`, else existing `nx_astar_search`.

### 8.3 LM‑TAD Wrapper (critics/lmtad_wrapper.py)

Purpose: provide a stable API to score candidate next roads using surprisal from LM‑TAD located at `/home/matt/Dev/LMTAD` (e.g., `code/eval_porto.py` and related modules).

- API
  - `class LMTADSurprisalScorer:`
    - `__init__(repo_path: str, ckpt_path: str, device: str, block_size: int = 64, cache_kv: bool = True)`
    - `score_candidates(history_ids: List[int], candidate_ids: List[int], tod_bucket: Optional[int] = None, user_id: Optional[int] = None) -> np.ndarray`
      - Returns one surprisal value per candidate (higher means more anomalous/unlikely).
- Implementation notes
  - `sys.path.insert(0, f"{repo_path}/code")`, import model/tokenizer/loader used by LM‑TAD.
  - Load the LM‑TAD model weights from `ckpt_path`, set `eval()`, `no_grad()`, FP16 if supported.
  - Use sliding window truncation to `block_size` for the history; cache KV states keyed by `(last_k_history_ids, tod_bucket, user_id)` to reduce latency.
  - If the LM‑TAD training vocabulary uses different road IDs, support a 1‑1 remap (see §8.4). For unknown tokens, fall back to a neutral median penalty.
  - Expose simple telemetry: cache hit/miss counters, average scoring latency.

### 8.4 Token and Context Alignment

- Token identity: Prefer using `rid_list` road IDs directly if LM‑TAD was trained on the same IDs. If not, provide a JSON map `./data/<dataset>/rid_to_lmtad_id.json` with `{ "<road_id>": <lmtad_id>, ... }` and apply it inside the wrapper.
- Time context: If LM‑TAD supports contextual tokens, derive a coarse time‑of‑day bucket (e.g., 0–23 or 0–5 bins) and a weekend flag. Pass them to the scorer; otherwise ignore.
- User context: Optional; default to `None` since HOSER generation doesn’t carry per‑user IDs.

### 8.5 History‑Aware A* with LM‑TAD Surprisal

New method inside `Searcher`:

- Signature: `astar_with_critic(origin_road_id, origin_datetime, destination_road_id, *, lambda_surprisal: float = 1e-2, w_length: float = 1.0, max_search_step: int = 5000)`
- Data sources: reuse `self.nx_graph` (edge weights), `self.road_center_gps` (heuristic), and date/time utilities already present.
- Step cost from `u -> v` given history `H`:
  - `base = w_length * edge_weight(u, v)` (prefers road length; fallback: center‑to‑center haversine)
  - `penalty = lambda_surprisal * surprisal_LMTAD(H, v)` where `surprisal_LMTAD` is from the wrapper’s `score_candidates([H], [v])`
  - `g_new = g[u] + base + penalty`
  - `f = g_new + heuristic(v, dest)`; heuristic remains purely geometric and admissible
- Expansion:
  - At each pop, get neighbors from `self.reachable_road_id_dict[u]` (skip dead‑ends).
  - Batch one LM‑TAD call per expansion: `score_candidates(history_ids, neighbors)`.
  - Update PQ with `(f, state)` where state stores `(node_id, g, parent, history)`.
  - Track `max_search_step` and a visited set keyed by `node_id` with `best_g` domination.
- Path reconstruction: on reaching `dest` (or on termination), backtrack parents to get the road sequence.
- Timestamping: After path is chosen, reuse the existing single‑candidate timestamp loop from `nx_astar_search` (unchanged) to produce `trace_datetime` with HOSER.

Pseudocode sketch:

```python
def astar_with_critic(o, t0, d, lambda_surprisal=1e-2, w_length=1.0, max_search_step=5000):
    def heuristic(n):
        return haversine(self.road_center_gps[n], self.road_center_gps[d], unit='m')

    frontier = PriorityQueue()
    frontier.put((0.0, o))
    g_cost, parent, history = {o: 0.0}, {o: None}, {o: [o]}
    steps = 0

    while not frontier.empty() and steps < max_search_step:
        _, u = frontier.get()
        if u == d:
            break

        neighbors = self.reachable_road_id_dict.get(u, [])
        if not neighbors:
            continue

        # LM‑TAD surprisal for all neighbors at once
        surprisal = scorer.score_candidates(history[u][-block_size:], neighbors, tod_bucket=..., user_id=None)

        for j, v in enumerate(neighbors):
            base = edge_weight(u, v)  # from self.nx_graph, fallback to haversine
            penalty = lambda_surprisal * float(surprisal[j])
            g_new = g_cost[u] + w_length * base + penalty
            if v not in g_cost or g_new < g_cost[v]:
                g_cost[v] = g_new
                parent[v] = u
                history[v] = history[u] + [v]
                f = g_new + heuristic(v)
                frontier.put((f, v))
        steps += 1

    path = backtrack(parent, o, d)
    return timestamp_with_hoser(path, t0)
```

### 8.6 CLI and Configuration

Add flags to `gene.py`:

- `--use_critic` — enable LM‑TAD‑augmented A* (otherwise use vanilla `nx_astar_search`)
- `--critic_repo /home/matt/Dev/LMTAD` — LM‑TAD repository root
- `--critic_ckpt /path/to/weights.pt` — LM‑TAD checkpoint
- `--critic_lambda 0.01` — weight for surprisal penalty
- `--critic_block_size 64` — history window length
- `--critic_gpu 0` — device index for LM‑TAD

Main flow change:

- If `args.nx_astar and args.use_critic`: construct `LMTADSurprisalScorer` and call `Searcher.astar_with_critic(...)`.
- Else: keep existing `nx_astar_search` path.

### 8.7 Performance and VRAM Tactics

- Keep LM‑TAD on GPU during search; keep HOSER on CPU until the path is found, then move HOSER to GPU for timestamping. This avoids peak overlap.
- Use FP16 inference, `torch.no_grad()`, and `block_size ≤ 64`.
- Enable KV caching in the wrapper; histories in A* change slowly—high cache hit rate expected.
- Optional: `torch.compile` on both models if supported.

### 8.8 Validation and Tuning

- A/B test on held‑out OD pairs:
  - Route‑level plausibility: mean LM‑TAD perplexity along the chosen route.
  - Downstream metrics vs real trips with same OD: DTW, EDR, and distributional JSD on duration/length.
- Sweep `--critic_lambda` in log‑scale: `1e-3, 3e-3, 1e-2, 3e-2`.
- Telemetry to log: cache hit rate, LM‑TAD ms per expansion, expansions per trajectory.

### 8.9 Optional: Training‑Time Critic Loss (Later)

- In `train.py`, with the critic frozen, add: `loss += lambda_train * surprisal(selected_next_road | history)` per step.
- Compute surprisal on short windows to bound cost; gate with a config flag. This pushes HOSER to align with LM‑TAD’s notion of normality during training.

### 8.10 Usage Example

Assuming an LM‑TAD checkpoint is available:

```bash
uv run python gene.py --dataset Beijing --cuda 0 --nx_astar --use_critic \
  --critic_repo /home/matt/Dev/LMTAD \
  --critic_ckpt /home/matt/Dev/LMTAD/checkpoints/porto_lmtad.pt \
  --critic_lambda 0.01 --critic_block_size 64 --critic_gpu 0
```

### 8.11 Risks and Mitigations

- Dynamic edge costs: NetworkX’s built‑in A* cannot use history‑dependent weights. Solution: implement a small, custom history‑aware A* in `Searcher` while still reusing the prebuilt graph and heuristic.
- Token mismatch: If LM‑TAD uses different IDs, provide `rid_to_lmtad_id.json`; default to neutral penalties where mapping is missing.
- Throughput: LM‑TAD scoring is amortized via a single batched call per expansion plus KV caching; keep `block_size` small.
- Fallbacks: On LM‑TAD errors/unavailability, automatically degrade to vanilla `nx_astar_search`.

### 8.12 Deliverables Checklist

- `critics/lmtad_wrapper.py` (LM‑TAD scorer with KV caching and optional ID remap)
- `gene.py` updates (CLI flags, `Searcher.astar_with_critic`, main flow branch)
- Optional: `./data/<dataset>/rid_to_lmtad_id.json` (ID mapping)
