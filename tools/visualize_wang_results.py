#!/usr/bin/env python3
"""
Wang Statistical Abnormality Detection - Visualization Generator

This script generates publication-quality visualizations from the aggregated
Wang abnormality detection results.

Usage:
    uv run python tools/visualize_wang_results.py
"""

import json
import logging
from pathlib import Path
from typing import Dict, Optional

import matplotlib.pyplot as plt
import numpy as np

# Import model detection utility
from tools.model_detection import MODEL_COLORS, get_model_color

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Set publication-quality defaults
plt.rcParams.update(
    {
        "font.size": 11,
        "axes.labelsize": 12,
        "axes.titlesize": 14,
        "xtick.labelsize": 10,
        "ytick.labelsize": 10,
        "legend.fontsize": 10,
        "figure.titlesize": 16,
        "font.family": "sans-serif",
    }
)

# Color scheme (imported from model_detection utility for consistency)
# Use get_model_color() function or MODEL_COLORS dict
COLORS = MODEL_COLORS  # For backward compatibility

PATTERN_COLORS = {
    "Abp1_normal": "#2ecc71",  # Green
    "Abp2_temporal_delay": "#f39c12",  # Orange
    "Abp3_route_deviation": "#e74c3c",  # Red
    "Abp4_both_deviations": "#8e44ad",  # Purple
}


def load_aggregated_results(json_path: Path) -> Dict:
    """Load aggregated results from JSON file"""
    with open(json_path, "r") as f:
        return json.load(f)


def plot_abnormality_rates_comparison(results: Dict, output_dir: Path, dataset: str):
    """Plot real vs generated abnormality rates for each model with confidence intervals"""
    summary = results.get("summary_statistics", {}).get(dataset, {})
    if not summary:
        logger.warning(f"No summary statistics for {dataset}")
        return

    real_rate = summary.get("mean_real_rate", 0)

    # Get model rates, CIs, and effect sizes from statistical tests
    tests = (
        results.get("statistical_analysis", {})
        .get("statistical_tests", {})
        .get(dataset, [])
    )

    model_data = {}
    for test in tests:
        model = test.get("model")
        split = test.get("od_source", "unknown")
        key = f"{model}_{split}"

        rate = test.get("generated_rate", 0)
        ci = test.get("generated_ci_95", [rate, rate])
        effect_size = test.get("effect_size", "unknown")
        cohens_h = test.get("cohens_h", 0)

        # Calculate error bar sizes (distance from rate to CI bounds)
        error_lower = rate - ci[0]
        error_upper = ci[1] - rate

        model_data[key] = {
            "rate": rate,
            "error_lower": error_lower,
            "error_upper": error_upper,
            "effect_size": effect_size,
            "cohens_h": cohens_h,
        }

    if not model_data:
        logger.warning(f"No model data found for {dataset}")
        return

    # Sort by rate
    sorted_models = sorted(model_data.items(), key=lambda x: x[1]["rate"])

    logger.info(f"  Plotting {len(sorted_models)} models for {dataset}")

    fig, ax = plt.subplots(figsize=(12, max(6, len(sorted_models) * 0.5)))
    models = [m[0] for m in sorted_models]
    rates = [m[1]["rate"] for m in sorted_models]
    error_lowers = [m[1]["error_lower"] for m in sorted_models]
    error_uppers = [m[1]["error_upper"] for m in sorted_models]
    effect_sizes = [m[1]["effect_size"] for m in sorted_models]

    # Create error bars in correct format (2xN array)
    errors = np.array([error_lowers, error_uppers])

    # Plot bars with error bars
    # Extract model names without test/train suffix for color lookup
    def get_model_color(model_key):
        # Remove _test or _train suffix
        model_name = model_key.rsplit("_test", 1)[0].rsplit("_train", 1)[0]
        # Simple two-color scheme: blue for distilled, red for vanilla
        if model_name.startswith("distill"):
            return "#3498db"  # Blue for all distilled models
        elif model_name.startswith("vanilla"):
            return "#e74c3c"  # Red for all vanilla models
        return "#95a5a6"  # Grey fallback

    colors = [get_model_color(m) for m in models]
    bars = ax.barh(
        models,
        rates,
        xerr=errors,
        color=colors,
        alpha=0.8,
        error_kw={"elinewidth": 2, "capsize": 4, "capthick": 2, "alpha": 0.7},
    )

    # Add value labels on bars, color-coded by effect size
    effect_colors_text = {"small": "green", "medium": "orange", "large": "red"}

    for i, (bar, rate, error_upper, effect) in enumerate(
        zip(bars, rates, error_uppers, effect_sizes)
    ):
        width = bar.get_width()
        color = effect_colors_text.get(effect, "black")

        # Rate text color-coded by effect size (green=good, orange=medium, red=poor)
        ax.text(
            width + error_upper + 0.5,
            bar.get_y() + bar.get_height() / 2,
            f"{rate:.2f}%",
            ha="left",
            va="center",
            fontsize=9,
            fontweight="bold",
            color=color,
        )

    # Add real rate line
    ax.axvline(
        real_rate,
        color=COLORS["real"],
        linestyle="--",
        linewidth=2,
        label=f"Real Data ({real_rate:.2f}%)",
    )

    ax.set_xlabel("Abnormality Rate (%) with 95% CI", fontsize=12)
    ax.set_ylabel("Model", fontsize=12)
    ax.set_title(
        f"Abnormality Rates: {dataset} - Real vs Generated\n(Error bars show 95% confidence intervals)",
        fontsize=14,
        fontweight="bold",
    )

    # Enhanced legend
    legend_elements = [
        plt.Line2D(
            [0],
            [0],
            color=COLORS["real"],
            linestyle="--",
            linewidth=2,
            label=f"Real Data Baseline ({real_rate:.2f}%)",
        ),
        plt.Line2D(
            [0],
            [0],
            marker="",
            linestyle="",
            label="",  # Spacer
            color="white",
        ),
        plt.Line2D(
            [0],
            [0],
            marker="",
            linestyle="",
            label="Bar Color = Model Type:",
            color="black",
        ),
        plt.Rectangle(
            (0, 0),
            1,
            1,
            fc=COLORS.get("distill", "#3498db"),
            alpha=0.8,
            label="  Distilled Models",
        ),
        plt.Rectangle(
            (0, 0), 1, 1, fc=COLORS["vanilla"], alpha=0.8, label="  Vanilla Models"
        ),
        plt.Line2D(
            [0],
            [0],
            marker="",
            linestyle="",
            label="",  # Spacer
            color="white",
        ),
        plt.Line2D(
            [0],
            [0],
            marker="",
            linestyle="",
            label="Label Color = Deviation from Real:",
            color="black",
        ),
        plt.Line2D(
            [0],
            [0],
            marker="o",
            linestyle="",
            markersize=8,
            markerfacecolor="green",
            markeredgecolor="green",
            label="  Green = Small deviation (realistic)",
        ),
        plt.Line2D(
            [0],
            [0],
            marker="o",
            linestyle="",
            markersize=8,
            markerfacecolor="orange",
            markeredgecolor="orange",
            label="  Orange = Medium deviation",
        ),
        plt.Line2D(
            [0],
            [0],
            marker="o",
            linestyle="",
            markersize=8,
            markerfacecolor="red",
            markeredgecolor="red",
            label="  Red = Large deviation (unrealistic)",
        ),
    ]
    ax.legend(handles=legend_elements, loc="best", fontsize=9)
    ax.grid(axis="x", alpha=0.3)

    plt.tight_layout()
    plt.savefig(
        output_dir / f"abnormality_rates_{dataset.lower()}.png",
        dpi=300,
        bbox_inches="tight",
    )
    plt.savefig(
        output_dir / f"abnormality_rates_{dataset.lower()}.svg", bbox_inches="tight"
    )
    plt.close()

    logger.info(f"âœ… Saved: abnormality_rates_{dataset.lower()}.png")


def plot_pattern_distribution(results: Dict, output_dir: Path):
    """Plot pattern distribution (Abp1-4) for each dataset as separate files"""
    pattern_data = results.get("statistical_analysis", {}).get(
        "pattern_distributions", {}
    )

    if not pattern_data:
        logger.warning("No pattern distribution data found")
        return

    datasets = ["Beijing", "porto_hoser", "BJUT_Beijing"]
    dataset_labels = {
        "Beijing": "Beijing",
        "porto_hoser": "Porto",
        "BJUT_Beijing": "BJUT Beijing",
    }

    # Generate separate plot for each dataset
    for dataset in datasets:
        data = pattern_data.get(dataset, {})
        if not data:
            logger.info(
                f"  No pattern distribution data for {dataset_labels[dataset]}, skipping"
            )
            continue

        fig, ax = plt.subplots(figsize=(8, 6))

        percentages = data.get("pattern_percentages", {})
        patterns = [
            "Abp1_normal",
            "Abp2_temporal_delay",
            "Abp3_route_deviation",
            "Abp4_both_deviations",
        ]
        pattern_labels = [
            "Normal",
            "Temporal Delay",
            "Route Deviation",
            "Both Deviations",
        ]
        values = [percentages.get(p, 0) for p in patterns]
        colors = [PATTERN_COLORS.get(p, "#95a5a6") for p in patterns]

        bars = ax.bar(pattern_labels, values, color=colors)
        ax.set_ylabel("Percentage (%)", fontsize=12)
        ax.set_title(
            f"{dataset_labels[dataset]} - Pattern Distribution",
            fontsize=14,
            fontweight="bold",
        )
        ax.set_ylim(0, max(values) * 1.1 if values and max(values) > 0 else 100)
        ax.grid(axis="y", alpha=0.3)

        # Add value labels on bars
        for bar, val in zip(bars, values):
            if val > 1:  # Only label if >1%
                height = bar.get_height()
                ax.text(
                    bar.get_x() + bar.get_width() / 2,
                    height + 0.5,
                    f"{val:.1f}%",
                    ha="center",
                    va="bottom",
                    fontsize=9,
                )

        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha="right")
        plt.tight_layout()

        # Save separate files for each dataset
        filename_base = f"pattern_distribution_{dataset}"
        plt.savefig(output_dir / f"{filename_base}.png", dpi=300, bbox_inches="tight")
        plt.savefig(output_dir / f"{filename_base}.svg", bbox_inches="tight")
        plt.close()

        logger.info(f"âœ… Saved: {filename_base}.png")


def plot_model_rankings(results: Dict, output_dir: Path):
    """Plot model rankings by realism (deviation from real rate) with confidence intervals as separate files"""
    rankings = results.get("statistical_analysis", {}).get("model_rankings", {})
    tests = results.get("statistical_analysis", {}).get("statistical_tests", {})

    if not rankings:
        logger.warning("No model rankings found")
        return

    datasets = ["Beijing", "porto_hoser", "BJUT_Beijing"]
    dataset_labels = {
        "Beijing": "Beijing",
        "porto_hoser": "Porto",
        "BJUT_Beijing": "BJUT Beijing",
    }
    od_sources = ["test", "train"]

    # Generate separate plot for each dataset and OD source (test/train)
    for dataset in datasets:
        dataset_rankings = rankings.get(dataset, [])
        dataset_tests = tests.get(dataset, [])

        if not dataset_rankings:
            logger.info(f"  No model rankings for {dataset_labels[dataset]}, skipping")
            continue

        for od_source in od_sources:
            # Filter rankings for this OD source
            source_rankings = [
                r for r in dataset_rankings if r["od_source"] == od_source
            ]

            if not source_rankings:
                logger.info(
                    f"  No {od_source} rankings for {dataset_labels[dataset]}, skipping"
                )
                continue

            fig, ax = plt.subplots(figsize=(10, 6))

            # Get top 6 models for this source
            top_models = source_rankings[:6]
            models = [m["model"] for m in top_models]  # No need for od_source in label
            real_rates = [m["real_rate"] for m in top_models]
            gen_rates = [m["generated_rate"] for m in top_models]

            # Get CIs from statistical tests
            real_cis = []
            gen_cis = []
            for m in top_models:
                # Find matching test
                test = next(
                    (
                        t
                        for t in dataset_tests
                        if t["model"] == m["model"] and t["od_source"] == m["od_source"]
                    ),
                    None,
                )
                if test:
                    real_ci = test.get("real_ci_95", [m["real_rate"], m["real_rate"]])
                    gen_ci = test.get(
                        "generated_ci_95", [m["generated_rate"], m["generated_rate"]]
                    )
                    real_cis.append(
                        [m["real_rate"] - real_ci[0], real_ci[1] - m["real_rate"]]
                    )
                    gen_cis.append(
                        [
                            m["generated_rate"] - gen_ci[0],
                            gen_ci[1] - m["generated_rate"],
                        ]
                    )
                else:
                    real_cis.append([0, 0])
                    gen_cis.append([0, 0])

            x = np.arange(len(models))
            width = 0.35

            # Convert CIs to format expected by matplotlib (2xN array)
            real_errors = np.array(real_cis).T
            gen_errors = np.array(gen_cis).T

            ax.bar(
                x - width / 2,
                real_rates,
                width,
                yerr=real_errors,
                label="Real (with 95% CI)",
                color=COLORS["real"],
                alpha=0.8,
                error_kw={"elinewidth": 2, "capsize": 4, "capthick": 2, "alpha": 0.7},
            )
            ax.bar(
                x + width / 2,
                gen_rates,
                width,
                yerr=gen_errors,
                label="Generated (with 95% CI)",
                color=COLORS.get("distilled", "#3498db"),
                alpha=0.8,
                error_kw={"elinewidth": 2, "capsize": 4, "capthick": 2, "alpha": 0.7},
            )

            ax.set_ylabel("Abnormality Rate (%) with 95% CI", fontsize=12)
            ax.set_title(
                f"{dataset_labels[dataset]} ({od_source.capitalize()}) - Top Models by Realism",
                fontsize=14,
                fontweight="bold",
            )
            ax.set_xticks(x)
            ax.set_xticklabels(models, rotation=45, ha="right", fontsize=9)
            ax.legend()
            ax.grid(axis="y", alpha=0.3)

            plt.tight_layout()

            # Save separate files for each dataset and od_source
            filename_base = f"model_rankings_{dataset}_{od_source}"
            plt.savefig(
                output_dir / f"{filename_base}.png", dpi=300, bbox_inches="tight"
            )
            plt.savefig(output_dir / f"{filename_base}.svg", bbox_inches="tight")
            plt.close()

            logger.info(f"âœ… Saved: {filename_base}.png")


def plot_cross_dataset_comparison(results: Dict, output_dir: Path):
    """Plot cross-dataset comparison (Beijing â†’ BJUT)"""
    cross_data = results.get("statistical_analysis", {}).get(
        "cross_dataset_comparison", {}
    )
    beijing_bjut = cross_data.get("beijing_to_bjut", {})

    if not beijing_bjut:
        logger.warning("No cross-dataset comparison data found")
        return

    models = list(beijing_bjut.keys())
    beijing_rates = [beijing_bjut[m]["beijing_rate"] for m in models]
    bjut_rates = [beijing_bjut[m]["bjut_rate"] for m in models]

    fig, ax = plt.subplots(figsize=(10, 6))

    x = np.arange(len(models))
    width = 0.35

    bars1 = ax.bar(
        x - width / 2,
        beijing_rates,
        width,
        label="Beijing (Same-Network)",
        color=COLORS.get("distilled", "#3498db"),
        alpha=0.8,
    )
    bars2 = ax.bar(
        x + width / 2,
        bjut_rates,
        width,
        label="BJUT (Cross-Network)",
        color=COLORS["vanilla"],
        alpha=0.8,
    )

    ax.set_ylabel("Abnormality Rate (%)", fontsize=12)
    ax.set_xlabel("Model", fontsize=12)
    ax.set_title(
        "Cross-Dataset Transfer: Beijing â†’ BJUT Beijing", fontsize=14, fontweight="bold"
    )
    ax.set_xticks(x)
    ax.set_xticklabels(models, rotation=45, ha="right")
    ax.legend()
    ax.grid(axis="y", alpha=0.3)

    # Add value labels
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(
                bar.get_x() + bar.get_width() / 2,
                height + 1,
                f"{height:.1f}%",
                ha="center",
                va="bottom",
                fontsize=9,
            )

    plt.tight_layout()
    plt.savefig(
        output_dir / "cross_dataset_comparison.png", dpi=300, bbox_inches="tight"
    )
    plt.savefig(output_dir / "cross_dataset_comparison.svg", bbox_inches="tight")
    plt.close()

    logger.info("âœ… Saved: cross_dataset_comparison.png")


def plot_statistical_significance(results: Dict, output_dir: Path):
    """Plot statistical significance test results (p-values) with effect sizes as separate files"""
    tests = results.get("statistical_analysis", {}).get("statistical_tests", {})

    if not tests:
        logger.warning("No statistical test data found")
        return

    datasets = ["Beijing", "porto_hoser", "BJUT_Beijing"]
    dataset_labels = {
        "Beijing": "Beijing",
        "porto_hoser": "Porto",
        "BJUT_Beijing": "BJUT Beijing",
    }
    od_sources = ["test", "train"]

    # Effect size color mapping
    effect_colors = {
        "small": "#2ecc71",  # Green
        "medium": "#f39c12",  # Orange
        "large": "#e74c3c",  # Red
    }

    # Generate separate plot for each dataset and OD source (test/train)
    for dataset in datasets:
        dataset_tests = tests.get(dataset, [])
        if not dataset_tests:
            logger.info(
                f"  No statistical tests for {dataset_labels[dataset]}, skipping"
            )
            continue

        for od_source in od_sources:
            # Filter tests for this OD source
            source_tests = [t for t in dataset_tests if t.get("od_source") == od_source]

            if not source_tests:
                logger.info(
                    f"  No {od_source} tests for {dataset_labels[dataset]}, skipping"
                )
                continue

            # Filter out tests with errors (missing p_value key)
            valid_tests = [t for t in source_tests if "p_value" in t]

            if not valid_tests:
                logger.info(
                    f"  No valid {od_source} tests for {dataset_labels[dataset]} (all tests failed), skipping"
                )
                continue

            # Sort by p-value (ascending, so most significant first)
            # Lower p-value = more significant = larger -log10(p)
            valid_tests.sort(key=lambda t: t["p_value"])

            fig, ax = plt.subplots(figsize=(10, max(6, len(valid_tests) * 0.5)))

            # Get p-values, effect sizes, and Cohen's h (log scale)
            models = [t["model"] for t in valid_tests]  # No need for od_source in label
            p_values = [max(t["p_value"], 1e-300) for t in valid_tests]  # Avoid log(0)
            log_p_values = [-np.log10(p) for p in p_values]
            effect_sizes = [t.get("effect_size", "unknown") for t in valid_tests]
            cohens_h = [t.get("cohens_h", 0) for t in valid_tests]

            # Color bars by effect size
            bar_colors = [effect_colors.get(e, "#95a5a6") for e in effect_sizes]

            bars = ax.barh(models, log_p_values, color=bar_colors, alpha=0.8)

            # Add Cohen's h annotations on bars
            for bar, h, effect in zip(bars, cohens_h, effect_sizes):
                width = bar.get_width()
                if width > 0:  # Only annotate if bar is visible
                    ax.text(
                        width * 0.95,
                        bar.get_y() + bar.get_height() / 2,
                        f"h={abs(h):.2f}",
                        ha="right",
                        va="center",
                        fontsize=8,
                        color="white",
                        fontweight="bold",
                    )

            ax.axvline(
                -np.log10(0.001),
                color="black",
                linestyle="--",
                linewidth=2,
                alpha=0.5,
                label="p=0.001",
            )

            ax.set_xlabel("-log10(p-value)", fontsize=12)
            ax.set_ylabel("Model", fontsize=12)
            ax.set_title(
                f"{dataset_labels[dataset]} ({od_source.capitalize()}) - Statistical Significance\n(colored by effect size)",
                fontsize=14,
                fontweight="bold",
            )

            # Enhanced legend
            legend_elements = [
                plt.Line2D(
                    [0],
                    [0],
                    color="black",
                    linestyle="--",
                    linewidth=2,
                    alpha=0.5,
                    label="p=0.001",
                ),
                plt.Rectangle(
                    (0, 0),
                    1,
                    1,
                    fc=effect_colors["small"],
                    alpha=0.8,
                    label="Small effect (h<0.2)",
                ),
                plt.Rectangle(
                    (0, 0),
                    1,
                    1,
                    fc=effect_colors["medium"],
                    alpha=0.8,
                    label="Medium effect (0.2â‰¤h<0.5)",
                ),
                plt.Rectangle(
                    (0, 0),
                    1,
                    1,
                    fc=effect_colors["large"],
                    alpha=0.8,
                    label="Large effect (hâ‰¥0.5)",
                ),
            ]
            ax.legend(handles=legend_elements, fontsize=9, loc="best")
            ax.grid(axis="x", alpha=0.3)

            plt.tight_layout()

            # Save separate files for each dataset and od_source
            filename_base = f"statistical_significance_{dataset}_{od_source}"
            plt.savefig(
                output_dir / f"{filename_base}.png", dpi=300, bbox_inches="tight"
            )
            plt.savefig(output_dir / f"{filename_base}.svg", bbox_inches="tight")
            plt.close()

            logger.info(f"âœ… Saved: {filename_base}.png")


def generate_wang_visualizations(
    results_file: Optional[Path] = None, output_dir: Optional[Path] = None
) -> None:
    """
    Generate Wang abnormality detection visualizations from aggregated results.

    This function can be called programmatically from other scripts or via CLI.

    Args:
        results_file: Path to aggregated results JSON file. If None, uses
                     wang_results_aggregated.json in project root
        output_dir: Output directory for visualizations. If None, uses
                   figures/wang_abnormality/ in project root

    Example:
        >>> from tools.visualize_wang_results import generate_wang_visualizations
        >>> generate_wang_visualizations(
        ...     results_file=Path("eval_dir/wang_results.json"),
        ...     output_dir=Path("eval_dir/figures/wang_abnormality")
        ... )
    """
    project_root = Path(__file__).parent.parent

    # Default paths
    if results_file is None:
        results_file = project_root / "wang_results_aggregated.json"
    else:
        results_file = Path(results_file)

    if output_dir is None:
        output_dir = project_root / "figures" / "wang_abnormality"
    else:
        output_dir = Path(output_dir)

    if not results_file.exists():
        logger.error(f"Results file not found: {results_file}")
        logger.info("Run: uv run python tools/analyze_wang_results.py")
        return

    output_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"ðŸ“‚ Loading results from {results_file}")
    results = load_aggregated_results(results_file)

    logger.info("ðŸ“Š Generating visualizations...")

    # Generate all plots
    for dataset in ["Beijing", "porto_hoser", "BJUT_Beijing"]:
        plot_abnormality_rates_comparison(results, output_dir, dataset)

    plot_pattern_distribution(results, output_dir)
    plot_model_rankings(results, output_dir)
    plot_cross_dataset_comparison(results, output_dir)
    plot_statistical_significance(results, output_dir)

    logger.info(f"âœ… All visualizations saved to {output_dir}")
    logger.info("   Files (separate per dataset and train/test split):")
    logger.info("   - abnormality_rates_<dataset>.png/svg")
    logger.info("   - pattern_distribution_<dataset>.png/svg")
    logger.info("   - model_rankings_<dataset>_<test|train>.png/svg")
    logger.info("   - statistical_significance_<dataset>_<test|train>.png/svg")
    logger.info("   - cross_dataset_comparison.png/svg (cross-dataset only)")


def main():
    """Main CLI entry point"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Generate Wang abnormality detection visualizations",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Use default paths (wang_results_aggregated.json in project root)
  uv run python tools/visualize_wang_results.py

  # Specify custom input and output
  uv run python tools/visualize_wang_results.py \\
    --input custom_results.json \\
    --output-dir custom_figures/

  # Generate visualizations for specific evaluation directory
  uv run python tools/visualize_wang_results.py \\
    --input hoser-distill-optuna-porto-eval-xyz/wang_results_aggregated.json \\
    --output-dir hoser-distill-optuna-porto-eval-xyz/figures/wang_abnormality
        """,
    )

    parser.add_argument(
        "--input",
        type=Path,
        help="Input JSON file with aggregated results (default: wang_results_aggregated.json in project root)",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        help="Output directory for visualizations (default: figures/wang_abnormality/ in project root)",
    )

    args = parser.parse_args()

    generate_wang_visualizations(results_file=args.input, output_dir=args.output_dir)


if __name__ == "__main__":
    main()
