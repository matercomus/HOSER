#!/usr/bin/env python3
"""
Paired Statistical Tests for Model Comparisons

This module provides functions for performing paired statistical tests when comparing
the same trajectories (OD pairs) across different models (e.g., vanilla vs distilled).

When the same OD pairs are generated by multiple models and evaluated with the same
metrics, the observations are naturally paired. Using paired tests provides:
- Greater statistical power (more sensitive to detect true differences)
- Correct handling of within-subject correlation
- More appropriate statistical methodology

Key Functions:
- paired_ttest: Parametric paired t-test (assumes normality of differences)
- wilcoxon_signed_rank: Non-parametric alternative (no normality assumption)
- compare_models_paired: High-level function for complete paired comparison

Usage Example:
    # Load evaluation results for two models on same OD pairs
    vanilla_metrics = [...]  # List of metric values for vanilla model
    distilled_metrics = [...]  # List of metric values for distilled model

    # Perform paired comparison
    result = compare_models_paired(
        model1_values=vanilla_metrics,
        model2_values=distilled_metrics,
        model1_name="vanilla",
        model2_name="distilled",
        metric_name="Hausdorff_norm"
    )

Author: HOSER Project
Date: 2025-11-06
"""

import logging
from dataclasses import dataclass
from typing import List, Tuple, Dict, Optional
import numpy as np

try:
    from scipy import stats

    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False
    logging.warning("scipy not available - paired tests will not work")

logger = logging.getLogger(__name__)


@dataclass
class PairedTestResult:
    """Results from a paired statistical test"""

    test_name: str
    model1_name: str
    model2_name: str
    metric_name: str

    # Sample statistics
    n_pairs: int
    model1_mean: float
    model2_mean: float
    mean_difference: float
    std_difference: float

    # Test statistics
    test_statistic: float
    p_value: float
    significant: bool  # At α=0.05

    # Effect size
    cohens_d: Optional[float] = None

    # Additional info
    assumptions_met: Optional[Dict[str, bool]] = None
    warnings: Optional[List[str]] = None


def paired_ttest(
    model1_values: np.ndarray, model2_values: np.ndarray, alpha: float = 0.05
) -> Tuple[float, float, float, bool]:
    """
    Perform paired t-test comparing two sets of matched observations.

    Null hypothesis: The mean difference between paired observations is zero.
    Alternative: The mean difference is not zero (two-tailed).

    Assumptions:
    - Paired observations (same OD pairs across models)
    - Differences approximately normally distributed
    - Independence of pairs

    Args:
        model1_values: Array of metric values from model 1
        model2_values: Array of metric values from model 2
        alpha: Significance level (default 0.05)

    Returns:
        Tuple of (t_statistic, p_value, mean_difference, significant)
    """
    if not HAS_SCIPY:
        raise ImportError("scipy required for paired t-test")

    if len(model1_values) != len(model2_values):
        raise ValueError("Arrays must have same length for paired test")

    if len(model1_values) < 2:
        raise ValueError("Need at least 2 pairs for paired t-test")

    # Perform paired t-test
    t_stat, p_val = stats.ttest_rel(model1_values, model2_values)

    # Calculate mean difference
    differences = model1_values - model2_values
    mean_diff = np.mean(differences)

    # Check significance
    significant = p_val < alpha

    return float(t_stat), float(p_val), float(mean_diff), significant


def wilcoxon_signed_rank(
    model1_values: np.ndarray, model2_values: np.ndarray, alpha: float = 0.05
) -> Tuple[float, float, float, bool]:
    """
    Perform Wilcoxon signed-rank test (non-parametric paired test).

    This is a non-parametric alternative to paired t-test that doesn't assume
    normality. It tests whether the median of differences is zero.

    Null hypothesis: The median difference between paired observations is zero.
    Alternative: The median difference is not zero (two-tailed).

    Advantages over paired t-test:
    - No normality assumption
    - Robust to outliers
    - Appropriate for ordinal data

    Args:
        model1_values: Array of metric values from model 1
        model2_values: Array of metric values from model 2
        alpha: Significance level (default 0.05)

    Returns:
        Tuple of (w_statistic, p_value, median_difference, significant)
    """
    if not HAS_SCIPY:
        raise ImportError("scipy required for Wilcoxon test")

    if len(model1_values) != len(model2_values):
        raise ValueError("Arrays must have same length for paired test")

    if len(model1_values) < 2:
        raise ValueError("Need at least 2 pairs for Wilcoxon test")

    # Perform Wilcoxon signed-rank test
    w_stat, p_val = stats.wilcoxon(model1_values, model2_values)

    # Calculate median difference
    differences = model1_values - model2_values
    median_diff = np.median(differences)

    # Check significance
    significant = p_val < alpha

    return float(w_stat), float(p_val), float(median_diff), significant


def compute_cohens_d_paired(
    model1_values: np.ndarray, model2_values: np.ndarray
) -> float:
    """
    Compute Cohen's d effect size for paired data.

    For paired data, Cohen's d is calculated as:
    d = mean(differences) / std(differences)

    Interpretation:
    - |d| < 0.2: Small effect
    - 0.2 ≤ |d| < 0.5: Small to medium effect
    - 0.5 ≤ |d| < 0.8: Medium to large effect
    - |d| ≥ 0.8: Large effect

    Args:
        model1_values: Array of metric values from model 1
        model2_values: Array of metric values from model 2

    Returns:
        Cohen's d effect size
    """
    differences = model1_values - model2_values
    mean_diff = np.mean(differences)
    std_diff = np.std(differences, ddof=1)

    if std_diff == 0:
        return 0.0 if mean_diff == 0 else np.inf

    return mean_diff / std_diff


def check_normality(data: np.ndarray, alpha: float = 0.05) -> Tuple[bool, float]:
    """
    Check if data is approximately normally distributed using Shapiro-Wilk test.

    Args:
        data: Array of values to test
        alpha: Significance level (default 0.05)

    Returns:
        Tuple of (is_normal, p_value)
        - is_normal: True if data appears normal (p > alpha)
        - p_value: P-value from Shapiro-Wilk test
    """
    if not HAS_SCIPY:
        raise ImportError("scipy required for normality test")

    if len(data) < 3:
        logger.warning("Sample too small for normality test")
        return True, 1.0  # Assume normal for small samples

    if len(data) > 5000:
        logger.info("Large sample - normality test may be overly sensitive")

    stat, p_val = stats.shapiro(data)
    is_normal = p_val > alpha

    return is_normal, float(p_val)


def compare_models_paired(
    model1_values: List[float],
    model2_values: List[float],
    model1_name: str = "Model 1",
    model2_name: str = "Model 2",
    metric_name: str = "Metric",
    alpha: float = 0.05,
    check_assumptions: bool = True,
) -> PairedTestResult:
    """
    Perform comprehensive paired comparison between two models.

    This function:
    1. Validates input data
    2. Computes descriptive statistics
    3. Checks statistical assumptions (if requested)
    4. Performs both parametric (paired t-test) and non-parametric (Wilcoxon) tests
    5. Computes effect size (Cohen's d)
    6. Returns comprehensive results

    Args:
        model1_values: List/array of metric values from model 1
        model2_values: List/array of metric values from model 2
        model1_name: Name of model 1 (for reporting)
        model2_name: Name of model 2 (for reporting)
        metric_name: Name of metric being compared
        alpha: Significance level (default 0.05)
        check_assumptions: Whether to check statistical assumptions

    Returns:
        PairedTestResult object with complete comparison results
    """
    if not HAS_SCIPY:
        raise ImportError("scipy required for paired statistical tests")

    # Convert to numpy arrays
    arr1 = np.array(model1_values)
    arr2 = np.array(model2_values)

    # Validate input
    if len(arr1) != len(arr2):
        raise ValueError(
            f"Arrays must have same length for paired test "
            f"(got {len(arr1)} vs {len(arr2)})"
        )

    if len(arr1) < 2:
        raise ValueError("Need at least 2 pairs for statistical testing")

    # Remove any NaN or inf values
    valid_mask = np.isfinite(arr1) & np.isfinite(arr2)
    if not np.all(valid_mask):
        logger.warning(f"Removing {np.sum(~valid_mask)} pairs with NaN/inf values")
        arr1 = arr1[valid_mask]
        arr2 = arr2[valid_mask]

    if len(arr1) < 2:
        raise ValueError("Not enough valid pairs after filtering")

    # Compute descriptive statistics
    n_pairs = len(arr1)
    model1_mean = float(np.mean(arr1))
    model2_mean = float(np.mean(arr2))

    differences = arr1 - arr2
    mean_diff = float(np.mean(differences))
    std_diff = float(np.std(differences, ddof=1))

    # Check assumptions if requested
    assumptions_met = {}
    warnings = []

    if check_assumptions:
        # Check normality of differences
        is_normal, norm_p = check_normality(differences, alpha=alpha)
        assumptions_met["differences_normal"] = is_normal

        if not is_normal:
            warnings.append(
                f"Differences may not be normally distributed (Shapiro-Wilk p={norm_p:.4f}). "
                "Consider using Wilcoxon test instead of paired t-test."
            )

    # Compute effect size
    cohens_d = compute_cohens_d_paired(arr1, arr2)

    # Determine which test to use
    use_parametric = True
    test_name = "Paired t-test"

    if check_assumptions and not assumptions_met.get("differences_normal", True):
        # Use non-parametric test if normality violated
        use_parametric = False
        test_name = "Wilcoxon signed-rank test"
        warnings.append(
            "Using non-parametric Wilcoxon test due to non-normal differences"
        )

    # Perform test
    if use_parametric:
        t_stat, p_val, _, significant = paired_ttest(arr1, arr2, alpha=alpha)
        test_stat = t_stat
    else:
        w_stat, p_val, _, significant = wilcoxon_signed_rank(arr1, arr2, alpha=alpha)
        test_stat = w_stat

    # Create result object
    result = PairedTestResult(
        test_name=test_name,
        model1_name=model1_name,
        model2_name=model2_name,
        metric_name=metric_name,
        n_pairs=n_pairs,
        model1_mean=model1_mean,
        model2_mean=model2_mean,
        mean_difference=mean_diff,
        std_difference=std_diff,
        test_statistic=test_stat,
        p_value=p_val,
        significant=significant,
        cohens_d=cohens_d,
        assumptions_met=assumptions_met if check_assumptions else None,
        warnings=warnings if warnings else None,
    )

    return result


def format_paired_test_result(result: PairedTestResult) -> str:
    """
    Format PairedTestResult as a readable string.

    Args:
        result: PairedTestResult object

    Returns:
        Formatted string summary
    """
    lines = []
    lines.append(f"\n{'=' * 70}")
    lines.append(f"Paired Statistical Test: {result.test_name}")
    lines.append(f"{'=' * 70}")
    lines.append(f"Metric: {result.metric_name}")
    lines.append(f"Models: {result.model1_name} vs {result.model2_name}")
    lines.append(f"Sample size: {result.n_pairs} matched pairs")
    lines.append("")
    lines.append("Descriptive Statistics:")
    lines.append(f"  {result.model1_name} mean: {result.model1_mean:.4f}")
    lines.append(f"  {result.model2_name} mean: {result.model2_mean:.4f}")
    lines.append(f"  Mean difference: {result.mean_difference:.4f}")
    lines.append(f"  Std of differences: {result.std_difference:.4f}")
    lines.append("")
    lines.append("Test Results:")
    lines.append(f"  Test statistic: {result.test_statistic:.4f}")
    lines.append(f"  P-value: {result.p_value:.6f}")
    lines.append(f"  Significant (α=0.05): {'Yes' if result.significant else 'No'}")

    if result.cohens_d is not None:
        lines.append("")
        lines.append("Effect Size:")
        lines.append(f"  Cohen's d: {result.cohens_d:.4f}")

        # Interpret effect size
        abs_d = abs(result.cohens_d)
        if abs_d < 0.2:
            interpretation = "negligible"
        elif abs_d < 0.5:
            interpretation = "small"
        elif abs_d < 0.8:
            interpretation = "medium"
        else:
            interpretation = "large"
        lines.append(f"  Interpretation: {interpretation} effect")

    if result.warnings:
        lines.append("")
        lines.append("Warnings:")
        for warning in result.warnings:
            lines.append(f"  - {warning}")

    lines.append(f"{'=' * 70}\n")

    return "\n".join(lines)
