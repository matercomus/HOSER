road_network_encoder_config:
  road_id_emb_dim: 64
  len_emb_dim: 16
  type_emb_dim: 16
  lon_emb_dim: 16
  lat_emb_dim: 16
  intersection_emb_dim: 128
  zone_id_emb_dim: 128
  # determined at runtime
  road_id_num_embeddings:
  # determined at runtime
  type_num_embeddings:
  zone_id_num_embeddings: 300

# determined at runtime
road_network_encoder_feature:
  road_attr:
    len:
    type:
    lon:
    lat:
  road_edge_index:
  intersection_attr:
  zone_edge_index:
  zone_edge_weight:

trajectory_encoder_config:
  hidden_dim: 128
  num_heads: 2
  num_layers: 2
  dropout: 0.0
  max_len: 1024
  grad_checkpoint: false # Disabled: we have enough VRAM, this trades memory for speed

navigator_config:
  hidden_dim: 128

optimizer_config:
  max_epoch: 25
  batch_size: 256  # Increased now that candidate_top_k filtering is disabled
  accum_steps: 4   # Gradient accumulation for effective batch of 128
  learning_rate: 0.001
  weight_decay: 0.1
  warmup_ratio: 0.1
  max_norm: 1.0

# Distillation settings (used by train_with_distill.py)
distill:
  enable: true
  repo: /home/matt/Dev/LMTAD
  ckpt: /home/matt/Dev/LMTAD/code/results/LMTAD/beijing_hoser_reference/run_20250928_202718/outlier_False/n_layer_8_n_head_12_n_embd_768_lr_0.0003_integer_poe_False/weights_only.pt
  window: 4  # Much more aggressive - 4x original size
  lambda: 0.01
  temperature: 2.0
  grid_size: 0.001  # Based on LM-TAD training: grip_size="205 252" (no downsampling)
  downsample: 1     # No downsampling for this model

# Point to the original HOSER dataset (beijing_hoser_reference is LM-TAD format only)
data_dir: /home/matt/Dev/HOSER-dataset

# Dataloader performance knobs - optimized for small dataset
dataloader:
  num_workers: 16   # Reduced from 16 - small dataset doesn't need many workers
  pin_memory: true # Enable for faster GPU transfer
  prefetch_factor: 2  # Increased for better pipelining
  persistent_workers: true

# Data-specific knobs
data:
  candidate_top_k: 0  # Disabled for now - causes issues with distillation label remapping

# Training performance knobs
training:
  allow_tf32: true
  cudnn_benchmark: true
  torch_compile: true
  torch_compile_mode: max-autotune  # Options: default, reduce-overhead, max-autotune
  disable_cudagraphs: true

# Weights & Biases logging
wandb:
  enable: true
  project: hoser-distill-optuna  # Single project for both Optuna study and individual trials
  run_name: ''  # Auto-generated per trial: trial_{N:03d}_{vanilla|distilled}
  tags: [beijing, lmtad, distillation, optuna]

# Optuna hyperparameter tuning configuration
optuna:
  n_trials: 25  # 24h target: 3 startup + 22 trials @ 70% pruning
  max_epochs: 3  # 3 epochs = ~2.4h per trial (vs 19h for 25 epochs)
  skip_baseline: true  # Skip vanilla baseline for faster tuning
  
  # Pruner configuration (MedianPruner for aggressive early stopping)
  pruner:
    n_startup_trials: 3  # First 3 trials establish baseline (~7h)
    n_warmup_steps: 0    # NO warmup - prune immediately after epoch 1
    interval_steps: 1    # Check every epoch
  
  # Sampler configuration (GPSampler for Bayesian optimization)
  sampler:
    n_startup_trials: 8  # GP model starts after 8 trials (~19h)

