road_network_encoder_config:
  road_id_emb_dim: 64
  len_emb_dim: 16
  type_emb_dim: 16
  lon_emb_dim: 16
  lat_emb_dim: 16
  intersection_emb_dim: 128
  zone_id_emb_dim: 128
  # determined at runtime
  road_id_num_embeddings:
  # determined at runtime
  type_num_embeddings:
  zone_id_num_embeddings: 300

# determined at runtime
road_network_encoder_feature:
  road_attr:
    len:
    type:
    lon:
    lat:
  road_edge_index:
  intersection_attr:
  zone_edge_index:
  zone_edge_weight:

trajectory_encoder_config:
  hidden_dim: 128
  num_heads: 2
  num_layers: 2
  dropout: 0.0
  max_len: 1024
  grad_checkpoint: false # Disabled: we have enough VRAM, this trades memory for speed

navigator_config:
  hidden_dim: 128

optimizer_config:
  max_epoch: 25
  batch_size: 128
  accum_steps: 8
  learning_rate: 0.001
  weight_decay: 0.1
  warmup_ratio: 0.1
  max_norm: 1.0

# Distillation settings (used by train_with_distill.py)
distill:
  enable: true
  repo: /home/matt/Dev/LMTAD
  ckpt: /home/matt/Dev/LMTAD/code/results/LMTAD/beijing_hoser_reference/run_20250928_202718/outlier_False/n_layer_8_n_head_12_n_embd_768_lr_0.0003_integer_poe_False/weights_only.pt
  window: 4  # Much more aggressive - 4x original size
  lambda: 0.01
  temperature: 2.0
  grid_size: 0.001  # Based on LM-TAD training: grip_size="205 252" (no downsampling)
  downsample: 1     # No downsampling for this model

# Point to the original HOSER dataset (beijing_hoser_reference is LM-TAD format only)
data_dir: /home/matt/Dev/HOSER-dataset

# Dataloader performance knobs - optimized for streaming large dataset
dataloader:
  num_workers: 6  # Efficient with RAM-cached dataset (no disk I/O)
  pin_memory: false # Disabled: Relieves CPU memory pressure
  prefetch_factor: 16  # Balanced buffer for disk streaming
  persistent_workers: false  # Disabled: Avoid memory leaks with repeated torch.load

# Data-specific knobs
data:
  candidate_top_k: 64  # Cap candidates per timestep to prevent memory/collate explosion

# Training performance knobs
training:
  seed: 43 # Base seed for reproducibility
  allow_tf32: true
  cudnn_benchmark: true
  torch_compile: true
  torch_compile_mode: max-autotune  # Options: default, reduce-overhead, max-autotune
  disable_cudagraphs: true

# Performance profiling
profiling:
  enable: true        # Enable periodic performance logging
  log_interval: 1000

# Weights & Biases logging
wandb:
  enable: true
  project: hoser-distill-optuna-6  # Single project for both Optuna study and individual trials
  run_name: ''  # Auto-generated per trial: trial_{N:03d}_{vanilla|distilled}
  tags: [beijing, distillation, optuna]

# Optuna hyperparameter tuning configuration
optuna:
  n_trials: 12  # Focused hyperparameter search (~3 days for Phase 1)
  max_epochs: 8  # Enough to see convergence trends (8 epochs × 54min = ~7.3h per full trial)
  
  # Pruner configuration (HyperbandPruner with moderate early-stopping)
  pruner:
    min_resource: 5      # Minimum epochs before pruning (5 epochs = ~4.6h minimum)
    reduction_factor: 3  # Keeps top 1/3 of trials at each rung
  
  # Sampler configuration (CmaEsSampler - optimal for continuous/integer params)
  sampler:
    sampler_type: "cmaes"  # CmaEsSampler: best for 10-100 trials, all continuous/integer params
    seed: 43               # Sampler seed for reproducibility
    # Note: CmaEsSampler doesn't need n_startup_trials - starts optimizing from trial 0
    # All hyperparameters are now continuous/integer (no categorical)
    
  # Final evaluation run (after hyperparameter search completes)
  final_run:
    enabled: true        # Run final 25-epoch training with best hyperparameters
    max_epochs: 25       # Full convergence for proper evaluation (25 × 54min = ~23h)
    seeds: [42, 43, 44]  # Multiple seeds for statistical robustness (optional)

