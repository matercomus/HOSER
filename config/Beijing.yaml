road_network_encoder_config:
  road_id_emb_dim: 64
  len_emb_dim: 16
  type_emb_dim: 16
  lon_emb_dim: 16
  lat_emb_dim: 16
  intersection_emb_dim: 128
  zone_id_emb_dim: 128
  # determined at runtime
  road_id_num_embeddings:
  # determined at runtime
  type_num_embeddings:
  zone_id_num_embeddings: 300

# determined at runtime
road_network_encoder_feature:
  road_attr:
    len:
    type:
    lon:
    lat:
  road_edge_index:
  intersection_attr:
  zone_edge_index:
  zone_edge_weight:

trajectory_encoder_config:
  hidden_dim: 128
  num_heads: 2
  num_layers: 2
  dropout: 0.0
  max_len: 1024
  grad_checkpoint: false # Disabled: we have enough VRAM, this trades memory for speed

navigator_config:
  hidden_dim: 128

optimizer_config:
  max_epoch: 25
  batch_size: 64
  accum_steps: 8
  learning_rate: 0.001
  weight_decay: 0.1
  warmup_ratio: 0.1
  max_norm: 1.0

# Distillation settings (used by train_with_distill.py)
distill:
  enable: true
  repo: /home/matt/Dev/LMTAD
  ckpt: /home/matt/Dev/LMTAD/code/results/LMTAD/beijing_hoser_reference/run_20250928_202718/outlier_False/n_layer_8_n_head_12_n_embd_768_lr_0.0003_integer_poe_False/weights_only.pt
  window: 4  # Much more aggressive - 4x original size
  lambda: 0.01
  temperature: 2.0
  grid_size: 0.001  # Based on LM-TAD training: grip_size="205 252" (no downsampling)
  downsample: 1     # No downsampling for this model

# Point to the original HOSER dataset (beijing_hoser_reference is LM-TAD format only)
data_dir: /home/matt/Dev/HOSER-dataset

# Dataloader performance knobs - optimized for streaming large dataset
dataloader:
  num_workers: 6  # Reduced to avoid file descriptor exhaustion
  pin_memory: false # Disabled: Relieves CPU memory pressure
  prefetch_factor: 16  # Reduced since we're streaming from disk
  persistent_workers: false  # Disabled: Avoid memory leaks with repeated torch.load

# Data-specific knobs
data:
  candidate_top_k: 64  # Cap candidates per timestep to prevent memory/collate explosion

# Training performance knobs
training:
  seed: 43 # Base seed for reproducibility
  allow_tf32: true
  cudnn_benchmark: true
  torch_compile: true
  torch_compile_mode: max-autotune  # Options: default, reduce-overhead, max-autotune
  disable_cudagraphs: true

# Performance profiling
profiling:
  enable: true        # Enable periodic performance logging
  log_interval: 100

# Weights & Biases logging
wandb:
  enable: true
  project: hoser-distill-optuna-5  # Single project for both Optuna study and individual trials
  run_name: ''  # Auto-generated per trial: trial_{N:03d}_{vanilla|distilled}
  tags: [beijing, distillation, optuna]

# Optuna hyperparameter tuning configuration
optuna:
  n_trials: 25  # Optimized for 24-hour completion (~54 min/epoch baseline)
  max_epochs: 5  # Reduced for fast iteration (5 epochs Ã— 54min = ~4.5h per full trial)
  
  # Pruner configuration (HyperbandPruner with moderate early-stopping)
  pruner:
    min_resource: 3      # Minimum epochs before pruning (3 epochs = ~2.7h minimum)
    reduction_factor: 3  # Keeps top 1/3 of trials at each rung
  
  # Sampler configuration (TPESampler for robust conditional search)
  sampler:
    n_startup_trials: 5  # TPE starts after 5 SUCCESSFUL trials (failed/pruned don't count)
    seed: 43              # Sampler seed for reproducibility
    multivariate: true    # Use multivariate TPE for correlated parameters
    group: true           # Decompose search space for conditional params (distill_enable)

