road_network_encoder_config:
  road_id_emb_dim: 64
  len_emb_dim: 16
  type_emb_dim: 16
  lon_emb_dim: 16
  lat_emb_dim: 16
  intersection_emb_dim: 128
  zone_id_emb_dim: 128
  # determined at runtime
  road_id_num_embeddings:
  # determined at runtime
  type_num_embeddings:
  zone_id_num_embeddings: 300

# determined at runtime
road_network_encoder_feature:
  road_attr:
    len:
    type:
    lon:
    lat:
  road_edge_index:
  intersection_attr:
  zone_edge_index:
  zone_edge_weight:

trajectory_encoder_config:
  hidden_dim: 128
  num_heads: 2
  num_layers: 2
  dropout: 0.0
  max_len: 1024
  grad_checkpoint: false # Disabled: we have enough VRAM, this trades memory for speed

navigator_config:
  hidden_dim: 128

optimizer_config:
  max_epoch: 25
  batch_size: 64
  accum_steps: 8
  learning_rate: 0.001
  weight_decay: 0.1
  warmup_ratio: 0.1
  max_norm: 1.0

# Distillation settings (used by train_with_distill.py)
distill:
  enable: true
  repo: /home/matt/Dev/LMTAD
  ckpt: /home/matt/Dev/LMTAD/code/results/LMTAD/beijing_hoser_reference/run_20250928_202718/outlier_False/n_layer_8_n_head_12_n_embd_768_lr_0.0003_integer_poe_False/weights_only.pt
  window: 4  # Much more aggressive - 4x original size
  lambda: 0.01
  temperature: 2.0
  grid_size: 0.001  # Based on LM-TAD training: grip_size="205 252" (no downsampling)
  downsample: 1     # No downsampling for this model

# Point to the original HOSER dataset (beijing_hoser_reference is LM-TAD format only)
data_dir: /home/matt/Dev/HOSER-dataset

# Dataloader performance knobs - optimized for small dataset
dataloader:
  num_workers: 8
  pin_memory: false # Disabled: Relieves CPU memory pressure, the bottleneck in this case
  prefetch_factor: 8
  persistent_workers: true  # Reuse workers between epochs

# Data-specific knobs
data:
  candidate_top_k: 0  # Disabled for now - causes issues with distillation label remapping

# Training performance knobs
training:
  seed: 43 # Base seed for reproducibility
  allow_tf32: true
  cudnn_benchmark: true
  torch_compile: true
  torch_compile_mode: max-autotune  # Options: default, reduce-overhead, max-autotune
  disable_cudagraphs: true

# Performance profiling
profiling:
  enable: true        # Enable periodic performance logging
  log_interval: 100   # Log performance stats every 200 batches

# Weights & Biases logging
wandb:
  enable: true
  project: hoser-distill-optuna-5  # Single project for both Optuna study and individual trials
  run_name: ''  # Auto-generated per trial: trial_{N:03d}_{vanilla|distilled}
  tags: [beijing, distillation, optuna]

# Optuna hyperparameter tuning configuration
optuna:
  n_trials: 25  # Extended trials with meaningful epochs
  max_epochs: 10  # Full 10 epochs for meaningful results (~8h per trial)
  
  # Pruner configuration (HyperbandPruner with aggressive early-stopping)
  pruner:
    min_resource: 5      # Minimum epochs before a trial can be pruned
    reduction_factor: 3  # Aggressiveness of pruning (e.g., keeps top 1/3 of trials)
  
  # Sampler configuration (TPESampler for robust conditional search)
  sampler:
    n_startup_trials: 5  # TPE model starts after 10 random trials
    seed: 43              # Sampler seed for reproducibility
    multivariate: true    # Use multivariate TPE
    group: true           # Decompose search space for conditional params

