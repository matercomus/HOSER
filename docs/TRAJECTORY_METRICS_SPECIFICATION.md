# Trajectory-Level Metrics Specification

## Overview

This document specifies the format and structure of trajectory-level metrics files (`trajectory_metrics.json`) generated by the HOSER evaluation pipeline. These files enable paired statistical tests by providing per-trajectory comparison metrics.

## Purpose

Trajectory-level metrics support:
- **Paired Statistical Tests**: Compare models on the same OD pairs with greater statistical power
- **Granular Analysis**: Examine individual trajectory performance beyond aggregate metrics
- **Data Reproducibility**: Enable re-analysis without re-running expensive evaluations
- **Cross-Model Comparisons**: Match trajectories across different model evaluations

## File Location

Trajectory-level metrics are saved alongside evaluation results:

```
./eval/{timestamp}/
├── results.json              # Aggregate evaluation metrics
└── trajectory_metrics.json   # Trajectory-level metrics (NEW)
```

## File Format

### JSON Structure

```json
{
  "metadata": {
    "generated_file": "path/to/generated.csv",
    "real_data_file": "path/to/real/data.csv",
    "od_source": "train|test",
    "evaluation_timestamp": "ISO-8601 timestamp",
    "num_trajectory_comparisons": 1234,
    "grid_size": 0.001,
    "edr_eps": 100.0
  },
  "trajectory_metrics": [
    {
      "od_pair": [12345, 67890],
      "real_traj_idx": 0,
      "gen_traj_idx": 0,
      "hausdorff_km": 1.234,
      "hausdorff_norm": 0.123,
      "dtw_km": 2.345,
      "dtw_norm": 0.234,
      "edr": 0.345,
      "len_real": 10,
      "len_gen": 12
    },
    ...
  ]
}
```

### Metadata Fields

| Field | Type | Description |
|-------|------|-------------|
| `generated_file` | string | Path to generated trajectories CSV |
| `real_data_file` | string | Path to real trajectories CSV |
| `od_source` | string | OD source: "train" or "test" |
| `evaluation_timestamp` | string | ISO-8601 timestamp when evaluation was performed |
| `num_trajectory_comparisons` | integer | Total number of trajectory comparisons |
| `grid_size` | float | Grid size in degrees used for OD pair matching |
| `edr_eps` | float | EDR distance threshold in meters |

### Trajectory Metrics Fields

Each entry in `trajectory_metrics` array represents one trajectory comparison:

| Field | Type | Description |
|-------|------|-------------|
| `od_pair` | [int, int] | Grid cell IDs for (origin, destination) |
| `real_traj_idx` | integer | Index of real trajectory in dataset |
| `gen_traj_idx` | integer | Index of generated trajectory in dataset |
| `hausdorff_km` | float | Hausdorff distance in kilometers |
| `hausdorff_norm` | float | Hausdorff distance normalized by avg trajectory length |
| `dtw_km` | float | DTW distance in kilometers |
| `dtw_norm` | float | DTW distance normalized by avg trajectory length |
| `edr` | float | Edit Distance on Real sequence (0-1, lower is better) |
| `len_real` | integer | Number of points in real trajectory |
| `len_gen` | integer | Number of points in generated trajectory |

## OD Pair Matching

### Grid-Based Matching

OD pairs are matched using a grid system:

1. **Grid Cell Calculation**: GPS coordinates are converted to grid cells
   - Grid size typically 0.001° (~111m at Beijing latitude)
   - `grid_cell_id = x * img_height + y`

2. **OD Pair Key**: Tuple of (origin_grid_id, destination_grid_id)
   - Trajectories with same OD pair key are compared
   - Enables consistent matching across different models

3. **Pairing Strategy**: Within each OD pair group
   - Match i-th real trajectory with i-th generated trajectory
   - Use `min(len(real), len(gen))` comparisons per OD pair

### Example

```python
# Grid cell IDs
origin_grid_id = 12345
dest_grid_id = 67890

# OD pair stored as list for JSON serialization
od_pair = [12345, 67890]

# Matching trajectories
# All generated trajectories with od_pair = [12345, 67890]
# are compared with real trajectories with same OD pair
```

## Metric Descriptions

### Distance Metrics

**Hausdorff Distance (`hausdorff_km`, `hausdorff_norm`)**:
- Measures maximum deviation between trajectories
- Raw version (`_km`) in kilometers
- Normalized version (`_norm`) divided by average trajectory length
- **Use normalized for fair comparison** across different trajectory lengths

**DTW Distance (`dtw_km`, `dtw_norm`)**:
- Dynamic Time Warping distance
- Accounts for temporal misalignment
- Raw version (`_km`) in kilometers  
- Normalized version (`_norm`) divided by average trajectory length
- **Use normalized for fair comparison** across different trajectory lengths

**EDR (`edr`)**:
- Edit Distance on Real sequence
- Counts edits needed to transform one trajectory to another
- Already normalized by max(len_traj1, len_traj2)
- Range: [0, 1], lower is better
- Threshold: 100m by default (`edr_eps`)

### Length Fields

- `len_real`: Number of road segments in real trajectory
- `len_gen`: Number of road segments in generated trajectory
- Used for normalization and analysis of trajectory complexity

## Usage Examples

### Loading Trajectory Metrics

```python
import json
from pathlib import Path

def load_trajectory_metrics(eval_dir):
    """Load trajectory metrics from evaluation directory."""
    metrics_file = eval_dir / "trajectory_metrics.json"
    
    with open(metrics_file) as f:
        data = json.load(f)
    
    metadata = data["metadata"]
    metrics = data["trajectory_metrics"]
    
    return metadata, metrics
```

### Matching OD Pairs Across Models

```python
def match_trajectories(metrics1, metrics2):
    """Match trajectories by OD pair across two models."""
    # Build OD index for model 1
    od_index = {}
    for m in metrics1:
        od_pair = tuple(m["od_pair"])
        if od_pair not in od_index:
            od_index[od_pair] = []
        od_index[od_pair].append(m)
    
    # Match with model 2
    matched1, matched2 = [], []
    for m2 in metrics2:
        od_pair = tuple(m2["od_pair"])
        if od_pair in od_index and od_index[od_pair]:
            m1 = od_index[od_pair].pop(0)
            matched1.append(m1)
            matched2.append(m2)
    
    return matched1, matched2
```

### Extracting Metric Values for Analysis

```python
def extract_metric_values(matched_metrics, metric_name):
    """Extract values for a specific metric."""
    return [m[metric_name] for m in matched_metrics]

# Example: Get normalized Hausdorff distances
hausdorff_vals = extract_metric_values(matched1, "hausdorff_norm")
```

### Performing Paired Statistical Test

```python
from tools.paired_statistical_tests import compare_models_paired

# Extract metric values for both models
metric_name = "hausdorff_norm"
values1 = extract_metric_values(matched1, metric_name)
values2 = extract_metric_values(matched2, metric_name)

# Perform paired t-test
result = compare_models_paired(
    model1_values=values1,
    model2_values=values2,
    model1_name="vanilla",
    model2_name="distilled",
    metric_name=metric_name,
    alpha=0.05
)

print(f"P-value: {result.p_value}")
print(f"Cohen's d: {result.cohens_d}")
print(f"Significant: {result.significant}")
```

## Integration with Pipeline

### Generation

Trajectory metrics are automatically generated during evaluation:

```bash
# Run evaluation with trajectory metrics
cd eval-dir
uv run python ../python_pipeline.py --only base_eval
```

### Standalone Paired Analysis

Use the standalone tool to compare models:

```bash
uv run python tools/compare_models_paired_analysis.py \
  --eval-dirs eval/model1/eval/2025-01-01_12-00-00 eval/model2/eval/2025-01-01_12-00-00 \
  --model-names vanilla distilled \
  --output paired_results.json
```

### Pipeline Paired Analysis Phase

Run paired analysis as part of the pipeline:

```bash
cd eval-dir
uv run python ../python_pipeline.py --only paired_analysis
```

This phase:
- Auto-detects available models
- Loads trajectory metrics for all models
- Performs pairwise comparisons
- Saves results to `./paired_analysis/{od_source}/{model1_vs_model2}/`

## Best Practices

### 1. Use Normalized Metrics

For trajectory length-independent comparisons:
- ✅ Use `hausdorff_norm`, `dtw_norm`
- ❌ Avoid `hausdorff_km`, `dtw_km` (unless analyzing absolute distances)

### 2. Ensure OD Pair Consistency

For valid paired tests:
- Generate trajectories with same OD source (train or test)
- Use same `grid_size` across evaluations
- Verify matching OD pairs before comparison

### 3. Handle Missing Matches

Not all OD pairs may match:
```python
if len(matched1) < 10:
    print(f"Warning: Only {len(matched1)} matched pairs, results may not be reliable")
```

Minimum recommended: 30+ matched trajectory pairs for statistical validity

### 4. Check Metric Validity

Filter invalid or missing values:
```python
import numpy as np

valid_mask = np.isfinite(values1) & np.isfinite(values2)
values1_clean = np.array(values1)[valid_mask]
values2_clean = np.array(values2)[valid_mask]
```

## File Size Considerations

- **Typical size**: 50-500 KB per evaluation
- **For 1000 trajectories**: ~100-200 KB
- **Storage**: Negligible compared to CSV trajectory files

The small size makes it practical to store trajectory metrics for all evaluations without significant storage overhead.

## Backward Compatibility

- Existing `results.json` format is unchanged
- Trajectory metrics are stored in separate file
- Older evaluations without trajectory metrics can be re-run if needed
- Paired analysis gracefully skips when trajectory metrics are unavailable

## Related Documentation

- [Paired Statistical Tests Guide](PAIRED_STATISTICAL_TESTS_GUIDE.md)
- [Evaluation Pipeline Documentation](evaluation/README.md)
- [Model Comparison Guide](EVALUATION_COMPARISON.md)

## Version History

- **2025-11-06**: Initial specification (Issue #51)
  - Defined JSON structure
  - Documented all metric fields
  - Added usage examples
  - Integration with pipeline

---

For questions or suggestions about trajectory-level metrics, please open an issue on GitHub.

