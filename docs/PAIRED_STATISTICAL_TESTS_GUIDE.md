# Paired Statistical Tests Guide

**Date**: 2025-11-06  
**Issue**: #16 - Missing Paired Statistical Tests  
**Module**: `tools/paired_statistical_tests.py`

---

## Overview

This guide explains the implementation and usage of **paired statistical tests** for comparing trajectory generation models in the HOSER project.

### The Problem

Prior to this implementation, the project used **unpaired statistical tests** (e.g., independent samples t-test, Mann-Whitney U) when comparing metrics across different models. However, when the **same OD pairs** are generated by multiple models and evaluated, the data is **naturally paired** - each OD pair in the vanilla model has a corresponding OD pair in the distilled model.

Using **paired tests** is more appropriate because:
1. **Greater statistical power**: Paired tests are more sensitive to detecting true differences
2. **Correct methodology**: Accounts for within-subject correlation (same OD pairs)
3. **More valid conclusions**: Proper statistical approach for matched data

---

## When to Use Paired Tests

### Paired Data (Use Paired Tests)

Use paired tests when comparing:
- **Same OD pairs across different models**
  - Example: vanilla vs distilled, both generating trajectories for the same 1,000 OD pairs
  - Each trajectory in vanilla has a corresponding matched trajectory in distilled
  
- **Trajectory-level metrics**:
  - Hausdorff distance for each OD pair
  - DTW distance for each OD pair
  - EDR for each OD pair
  - Trajectory length
  - Duration
  - Any per-trajectory metric

### Unpaired Data (Use Unpaired Tests)

Use unpaired tests when comparing:
- **Real vs Generated trajectories**
  - Real trajectories come from dataset
  - Generated trajectories are model-produced
  - These are different sets of trajectories, not matched pairs

- **Different OD pair sets**
  - Model A generates 1,000 train OD pairs
  - Model B generates 1,000 test OD pairs
  - The OD pairs are different, so not paired

---

## Available Tests

### 1. Paired T-Test (Parametric)

**Function**: `paired_ttest()`

**When to use**:
- Differences between paired observations are approximately normally distributed
- Continuous data
- Sufficient sample size (n â‰¥ 30 recommended)

**Assumptions**:
- Paired observations (same OD pairs)
- Differences approximately normally distributed
- Independence of pairs

**Null Hypothesis**: Hâ‚€: Î¼_diff = 0 (mean difference is zero)

**Example**:
```python
from tools.paired_statistical_tests import paired_ttest

t_stat, p_value, mean_diff, significant = paired_ttest(
    vanilla_hausdorff,  # Array of Hausdorff values from vanilla
    distilled_hausdorff,  # Array of Hausdorff values from distilled (same OD pairs)
    alpha=0.05
)

if significant:
    print(f"Models differ significantly (p={p_value:.4f}, t={t_stat:.2f})")
    print(f"Mean difference: {mean_diff:.4f}")
```

### 2. Wilcoxon Signed-Rank Test (Non-Parametric)

**Function**: `wilcoxon_signed_rank()`

**When to use**:
- Differences are NOT normally distributed
- Small sample sizes
- Ordinal data
- Robust to outliers

**Advantages**:
- No normality assumption
- More robust to outliers
- Works with small samples

**Null Hypothesis**: Hâ‚€: median_diff = 0 (median difference is zero)

**Example**:
```python
from tools.paired_statistical_tests import wilcoxon_signed_rank

w_stat, p_value, median_diff, significant = wilcoxon_signed_rank(
    vanilla_edr,  # Array of EDR values from vanilla
    distilled_edr,  # Array of EDR values from distilled (same OD pairs)
    alpha=0.05
)

if significant:
    print(f"Models differ significantly (p={p_value:.4f}, W={w_stat:.2f})")
    print(f"Median difference: {median_diff:.4f}")
```

### 3. Comprehensive Paired Comparison

**Function**: `compare_models_paired()`

**Recommended** for most use cases. This function:
1. Validates input data
2. Computes descriptive statistics
3. Checks statistical assumptions (normality)
4. Automatically chooses appropriate test (parametric vs non-parametric)
5. Computes effect size (Cohen's d)
6. Returns comprehensive results

**Example**:
```python
from tools.paired_statistical_tests import compare_models_paired, format_paired_test_result

result = compare_models_paired(
    model1_values=vanilla_dtw,
    model2_values=distilled_dtw,
    model1_name="vanilla",
    model2_name="distilled",
    metric_name="DTW_norm",
    alpha=0.05,
    check_assumptions=True  # Automatically check normality
)

# Print formatted results
print(format_paired_test_result(result))

# Or access specific fields
print(f"Test: {result.test_name}")
print(f"P-value: {result.p_value:.6f}")
print(f"Effect size (Cohen's d): {result.cohens_d:.4f}")
print(f"Significant: {result.significant}")
```

---

## Effect Size: Cohen's d for Paired Data

**Formula**: d = mean(differences) / std(differences)

**Interpretation**:
- |d| < 0.2: **Negligible effect**
- 0.2 â‰¤ |d| < 0.5: **Small effect**
- 0.5 â‰¤ |d| < 0.8: **Medium effect**
- |d| â‰¥ 0.8: **Large effect**

**Why important**:
- Statistical significance (p-value) tells you IF there's a difference
- Effect size (Cohen's d) tells you HOW BIG the difference is
- Large samples can show statistical significance for trivial differences
- Effect size provides practical significance

**Example**:
```python
result = compare_models_paired(...)
print(f"Cohen's d: {result.cohens_d:.4f}")

if abs(result.cohens_d) >= 0.8:
    print("Large practical effect - substantial difference")
elif abs(result.cohens_d) >= 0.5:
    print("Medium practical effect - moderate difference")
elif abs(result.cohens_d) >= 0.2:
    print("Small practical effect - minor difference")
else:
    print("Negligible practical effect - trivial difference")
```

---

## Pipeline Integration

### Overview

Paired statistical tests are now fully integrated into the HOSER evaluation pipeline. Trajectory-level metrics are automatically saved during evaluation, enabling paired comparisons without manual data extraction.

### Automatic Trajectory-Level Metrics

When you run evaluations, the pipeline now generates two files:

1. **`results.json`**: Aggregate evaluation metrics (unchanged)
2. **`trajectory_metrics.json`**: Per-trajectory metrics for paired tests (NEW)

**Location**:
```
./eval/{timestamp}/
â”œâ”€â”€ results.json              # Aggregate metrics
â””â”€â”€ trajectory_metrics.json   # Trajectory-level metrics
```

**Contents** of `trajectory_metrics.json`:
```json
{
  "metadata": {
    "generated_file": "...",
    "od_source": "train",
    "num_trajectory_comparisons": 1234,
    "grid_size": 0.001,
    "edr_eps": 100.0
  },
  "trajectory_metrics": [
    {
      "od_pair": [12345, 67890],
      "hausdorff_norm": 0.123,
      "dtw_norm": 0.234,
      "edr": 0.345,
      "len_real": 10,
      "len_gen": 12
    },
    ...
  ]
}
```

See [TRAJECTORY_METRICS_SPECIFICATION.md](TRAJECTORY_METRICS_SPECIFICATION.md) for complete documentation.

### Running Paired Analysis via Pipeline

#### Method 1: As Part of Full Pipeline

The `paired_analysis` phase runs automatically after `base_eval`:

```bash
cd eval-dir
uv run python ../python_pipeline.py

# Output:
# âœ… Generation complete
# âœ… Base evaluation complete
# ðŸ“Š Running paired statistical analysis...
# âœ… Paired analysis complete! 3 comparisons saved to ./paired_analysis/
```

#### Method 2: Run Paired Analysis Only

If evaluations are already complete:

```bash
cd eval-dir
uv run python ../python_pipeline.py --only paired_analysis

# Output:
# ðŸ“Š Running paired statistical analysis...
# ðŸ“‚ Processing test OD
#   Comparing vanilla vs distilled_phase1
#   âœ“ hausdorff_norm: p=0.0012, d=0.456
#   âœ“ dtw_norm: p=0.0034, d=0.389
#   âœ“ edr: p=0.0089, d=0.312
#   âœ… Results saved to ./paired_analysis/test/vanilla_vs_distilled_phase1/
```

#### Method 3: Skip Paired Analysis

To disable paired analysis:

```bash
# Option 1: Skip via command line
cd eval-dir
uv run python ../python_pipeline.py --skip paired_analysis

# Option 2: Remove from config/evaluation.yaml
# Comment out or remove from phases list:
phases:
  - generation
  - base_eval
  # - paired_analysis  # DISABLED
  - cross_dataset
```

### Output Structure

Paired analysis results are saved to:

```
./paired_analysis/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ vanilla_vs_distilled_phase1/
â”‚   â”‚   â”œâ”€â”€ paired_comparison.json
â”‚   â”‚   â””â”€â”€ paired_comparison.md
â”‚   â””â”€â”€ vanilla_vs_distilled_phase2/
â”‚       â”œâ”€â”€ paired_comparison.json
â”‚       â””â”€â”€ paired_comparison.md
â””â”€â”€ test/
    â””â”€â”€ vanilla_vs_distilled_phase1/
        â”œâ”€â”€ paired_comparison.json
        â””â”€â”€ paired_comparison.md
```

**JSON Format**:
```json
{
  "model1_name": "vanilla",
  "model2_name": "distilled_phase1",
  "n_matched_pairs": 1234,
  "alpha": 0.05,
  "metrics": {
    "hausdorff_norm": {
      "test_name": "Paired t-test",
      "p_value": 0.0012,
      "cohens_d": 0.456,
      "significant": true,
      "model1_mean": 0.234,
      "model2_mean": 0.178
    },
    ...
  }
}
```

**Markdown Format**: Human-readable summary with interpretation

### Standalone Tool Usage

For more control, use the standalone comparison tool:

```bash
# Basic usage
uv run python tools/compare_models_paired_analysis.py \
  --eval-dirs eval/model1/eval/2025-01-01_12-00-00 eval/model2/eval/2025-01-01_12-00-00 \
  --model-names vanilla distilled \
  --output paired_results.json

# With custom metrics
uv run python tools/compare_models_paired_analysis.py \
  --eval-dirs dir1 dir2 \
  --model-names model1 model2 \
  --metrics hausdorff_norm dtw_norm edr \
  --alpha 0.01 \
  --output results.json
```

### Integration with Cross-Seed Analysis

The `cross_seed_analysis.py` script now includes optional paired tests:

```bash
uv run python scripts/analysis/cross_seed_analysis.py \
  --eval_dirs eval/model1/eval eval/model2/eval \
  --output_dir docs/results

# If trajectory_metrics.json files are available:
# âœ… Generates CROSS_SEED_ANALYSIS.md with paired tests section
# ðŸ“Š Compares models across all seeds using paired tests
```

The generated report includes:
- Cross-seed statistics (mean Â± std, CV)
- **Paired Statistical Tests** section (NEW)
  - Model comparisons on matched trajectories
  - P-values, Cohen's d, effect sizes
  - Aggregated across all available seeds

### Interpreting Pipeline Results

#### JSON Results

```python
import json

with open("paired_analysis/test/vanilla_vs_distilled/paired_comparison.json") as f:
    results = json.load(f)

# Check if difference is significant
for metric, data in results["metrics"].items():
    if data["significant"]:
        print(f"{metric}: Significant difference (p={data['p_value']:.4f})")
        print(f"  Cohen's d: {data['cohens_d']:.3f}")
        print(f"  {results['model1_name']}: {data['model1_mean']:.4f}")
        print(f"  {results['model2_name']}: {data['model2_mean']:.4f}")
```

#### Effect Size Interpretation

The pipeline automatically interprets Cohen's d:

| |d| Range | Effect Size | Interpretation |
|-----------|-------------|----------------|
| < 0.2 | Negligible | Difference exists but practically insignificant |
| 0.2 - 0.5 | Small | Noticeable but modest difference |
| 0.5 - 0.8 | Medium | Substantial, meaningful difference |
| â‰¥ 0.8 | Large | Very significant, impactful difference |

### Best Practices

#### 1. Ensure Trajectory Metrics Are Generated

Trajectory metrics are generated automatically, but verify:

```bash
# Check for trajectory_metrics.json
ls eval/*/trajectory_metrics.json

# If missing, re-run evaluation:
cd eval-dir
uv run python ../python_pipeline.py --only base_eval
```

#### 2. Use Same OD Source for Comparisons

```bash
# âœ… GOOD: Both models on same OD source
./eval/
â”œâ”€â”€ model1_test/  # Test OD
â””â”€â”€ model2_test/  # Test OD â†’ Paired comparison valid

# âŒ BAD: Different OD sources
./eval/
â”œâ”€â”€ model1_train/  # Train OD
â””â”€â”€ model2_test/   # Test OD â†’ Cannot be paired
```

#### 3. Verify Matching OD Pairs

```python
# Check matched pairs count
n_matched = results["n_matched_pairs"]
if n_matched < 30:
    print(f"Warning: Only {n_matched} matched pairs, results may not be reliable")
```

Minimum recommended: 30+ matched trajectory pairs for statistical validity

#### 4. Consider Multiple Comparisons

When comparing multiple metrics, use FDR correction (automatically applied in pipeline):

```python
from tools.paired_statistical_tests import apply_fdr_correction

# Results from multiple comparisons
p_values = [r["p_value"] for r in results["metrics"].values()]
metric_names = list(results["metrics"].keys())

corrected_results = apply_fdr_correction(p_values, metric_names, alpha=0.05)

for metric, corrected in corrected_results.items():
    print(f"{metric}: p_corrected={corrected['p_adjusted']:.4f}, "
          f"significant={corrected['significant']}")
```

### Troubleshooting

**Problem**: `trajectory_metrics.json not found`
- **Solution**: Run `--only base_eval` to generate trajectory metrics

**Problem**: `No matching trajectory pairs found`
- **Solution**: Ensure both models used same OD source (train or test)
- Check that evaluations used same `grid_size` parameter

**Problem**: Paired analysis shows "Only 1 model found for test OD, skipping comparisons"
- **Solution**: Ensure multiple models have been evaluated on the same OD source
- Check that evaluation results have proper `model_type` metadata

**Problem**: High p-values (not significant) despite visible differences
- **Check**: Sample size (`n_matched_pairs` should be â‰¥ 30)
- **Check**: Effect size (Cohen's d) - small effect sizes need larger samples
- **Consider**: Use Wilcoxon test if data is non-normal (automatically selected)

---

## Complete Usage Example

### Scenario: Comparing Vanilla vs Distilled on Same OD Pairs

```python
#!/usr/bin/env python3
"""Example: Paired comparison of vanilla vs distilled models"""

import json
from pathlib import Path
from tools.paired_statistical_tests import compare_models_paired, format_paired_test_result

# Load evaluation results (both models evaluated on same OD pairs)
with open("eval/vanilla_results.json") as f:
    vanilla_results = json.load(f)

with open("eval/distilled_results.json") as f:
    distilled_results = json.load(f)

# Extract per-OD-pair metrics (you would need to save these during evaluation)
# For demonstration, assume we have trajectory-level results
vanilla_metrics = {
    "Hausdorff_norm": [...],  # List of Hausdorff for each OD pair
    "DTW_norm": [...],        # List of DTW for each OD pair
    "EDR": [...]              # List of EDR for each OD pair
}

distilled_metrics = {
    "Hausdorff_norm": [...],
    "DTW_norm": [...],
    "EDR": [...]
}

# Perform paired comparison for each metric
metrics_to_compare = ["Hausdorff_norm", "DTW_norm", "EDR"]
results = {}

for metric in metrics_to_compare:
    print(f"\n{'='*70}")
    print(f"Comparing {metric}")
    print(f"{'='*70}")
    
    result = compare_models_paired(
        model1_values=vanilla_metrics[metric],
        model2_values=distilled_metrics[metric],
        model1_name="vanilla",
        model2_name="distilled",
        metric_name=metric,
        alpha=0.05,
        check_assumptions=True
    )
    
    results[metric] = result
    print(format_paired_test_result(result))

# Summary
print("\n" + "="*70)
print("SUMMARY")
print("="*70)

for metric, result in results.items():
    sig_str = "âœ“ Significant" if result.significant else "âœ— Not significant"
    effect_str = f"d={result.cohens_d:.3f}"
    print(f"{metric:20s}: {sig_str:20s} (p={result.p_value:.4f}, {effect_str})")
```

---

## Statistical Power and Sample Size

### Why Paired Tests Have Greater Power

Consider comparing Hausdorff distance between vanilla and distilled:

**Unpaired test**:
- Compares group means: E[Hausdorff_vanilla] vs E[Hausdorff_distilled]
- Must account for between-trajectory variance (different OD pairs have different characteristics)
- Larger variance â†’ harder to detect differences

**Paired test**:
- Compares within-pair differences: E[Hausdorff_vanilla - Hausdorff_distilled]
- Removes between-trajectory variance (each OD pair is its own control)
- Smaller variance â†’ easier to detect differences

**Example**:
- OD pair 1: vanilla=5.2, distilled=3.1 (difference=-2.1)
- OD pair 2: vanilla=2.4, distilled=0.8 (difference=-1.6)
- OD pair 3: vanilla=8.1, distilled=5.9 (difference=-2.2)

Paired test focuses on the **consistent negative differences** (-2.1, -1.6, -2.2), removing the large between-pair variability (2.4 to 8.1).

### Sample Size Considerations

- **Minimum**: n â‰¥ 2 pairs (technically possible, but not recommended)
- **Adequate**: n â‰¥ 30 pairs (sufficient for paired t-test by CLT)
- **Good**: n â‰¥ 100 pairs (robust results)
- **Excellent**: n â‰¥ 1000 pairs (current HOSER evaluations use 1000-5000)

With n=1000+ OD pairs, HOSER has **excellent statistical power** to detect even small effects.

---

## Multiple Testing Correction

When performing multiple paired comparisons (e.g., comparing 5 different metrics), apply **multiple testing correction** to control family-wise error rate.

**Options**:
1. **Bonferroni correction**: Î±_corrected = Î± / n_tests
   - Most conservative
   - Example: Î±=0.05, n=5 tests â†’ Î±_corrected = 0.01

2. **FDR (Benjamini-Hochberg)**: Controls false discovery rate
   - Less conservative than Bonferroni
   - Recommended for exploratory analyses

**Example**:
```python
from statsmodels.stats.multitest import multipletests

# Collect p-values from multiple paired tests
p_values = [result.p_value for result in results.values()]

# Apply FDR correction
reject, p_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')

# Report corrected results
for i, (metric, result) in enumerate(results.items()):
    print(f"{metric}: p_raw={result.p_value:.4f}, p_corrected={p_corrected[i]:.4f}, "
          f"significant={reject[i]}")
```

---

## Reporting in Papers

### Statistical Methods Section

> **Statistical Analysis**: We compared trajectory-level metrics between vanilla and distilled models using **paired statistical tests**. Since both models generated trajectories for the same origin-destination (OD) pairs, observations were naturally paired. For each metric (Hausdorff distance, DTW distance, EDR), we performed **paired t-tests** when differences were approximately normally distributed (Shapiro-Wilk p > 0.05) and **Wilcoxon signed-rank tests** otherwise. We report **Cohen's d effect sizes** to quantify practical significance. Multiple testing correction was applied using the **False Discovery Rate (FDR)** method with Î±=0.05.

### Results Reporting

> The distilled model showed significantly lower DTW distance compared to vanilla (paired t-test: t=-12.4, p<0.001, Cohen's d=-0.62, n=1000 OD pairs), representing a **medium-to-large practical effect**. Mean DTW was 0.31 for distilled vs 0.45 for vanilla, a reduction of 31% (95% CI: [28%, 34%]). Similar improvements were observed for Hausdorff distance (Wilcoxon: W=42,315, p<0.001, d=-0.51) and EDR (paired t-test: t=-8.9, p<0.001, d=-0.44). All results remained significant after FDR correction.

---

## Validation and Testing

The paired statistical tests implementation has been validated:

### Unit Tests
- âœ… Paired t-test produces correct results (verified against scipy)
- âœ… Wilcoxon test produces correct results
- âœ… Cohen's d calculation correct for paired data
- âœ… Normality checking works properly
- âœ… Appropriate test selection (parametric vs non-parametric)

### Integration Tests
- âœ… Works with real trajectory evaluation data
- âœ… Handles edge cases (NaN, inf values)
- âœ… Correct error handling for invalid inputs

### Cross-Validation
- âœ… Results match manual calculations
- âœ… Consistent with published examples
- âœ… Produces expected p-values and effect sizes

---

## Limitations and Assumptions

### Paired Tests Assume:
1. **Pairing is valid**: Same OD pairs must truly correspond across models
2. **Independence of pairs**: Different OD pairs are independent observations
3. **No systematic ordering**: Pair order doesn't affect differences

### Not Appropriate When:
- Comparing real vs generated data (different sets)
- Different OD pairs across models
- Aggregate statistics only (need trajectory-level data)

### Current Limitations:
- Requires trajectory-level metrics (not just aggregates)
- May need to update evaluation pipeline to save per-OD metrics
- Large memory requirements for 1000+ trajectory comparisons

---

## Next Steps

### Implementation Tasks:
1. âœ… **Create paired tests module** (`tools/paired_statistical_tests.py`)
2. âœ… **Update evaluation pipeline** to save trajectory-level metrics (`evaluation.py`)
3. âœ… **Integrate into analysis scripts** (`tools/compare_models_paired_analysis.py`, `cross_seed_analysis.py`)
4. âœ… **Add paired_analysis phase** to pipeline (`python_pipeline.py`)
5. âœ… **Create trajectory metrics specification** (`docs/TRAJECTORY_METRICS_SPECIFICATION.md`)
6. **Re-run statistical analyses** with paired tests (in progress)
7. **Update documentation** with results (in progress)
8. **Update papers/reports** with proper methodology (pending)

### Future Enhancements:
- Add bootstrap confidence intervals for effect sizes
- Implement Bayesian paired tests
- Add visualization functions for paired data
- Create automated reporting templates

---

## References

### Statistical Methods:
- **Paired t-test**: Student (1908), "The Probable Error of a Mean"
- **Wilcoxon signed-rank**: Wilcoxon (1945), "Individual Comparisons by Ranking Methods"
- **Cohen's d**: Cohen (1988), "Statistical Power Analysis for the Behavioral Sciences"
- **Effect size interpretation**: Sawilowsky (2009), "New Effect Size Rules of Thumb"

### Best Practices:
- **Paired vs Unpaired**: Sullivan & Feinn (2012), "Using Effect Sizeâ€”or Why the P Value Is Not Enough"
- **Multiple Testing**: Benjamini & Hochberg (1995), "Controlling the False Discovery Rate"
- **Power Analysis**: Cohen (1992), "A Power Primer"

---

## Support

For questions or issues with paired statistical tests:
- See Issue #16 on GitHub
- Contact: HOSER development team
- Documentation: This guide

---

**Last Updated**: 2025-11-06  
**Version**: 2.0  
**Status**: Pipeline Integration Complete (Issue #51)
