# Cross-Dataset Evaluation Analysis Comparison

**Date:** October 31, 2025  
**Purpose:** Compare Beijing and Porto evaluation analyses for completeness and consistency

---

## Document Structure Comparison

| Section | Beijing | Porto | Notes |
|---------|---------|-------|-------|
| **Executive Summary** | ‚úÖ | ‚úÖ | Both present |
| **1. Experimental Setup** | ‚úÖ | ‚úÖ | Both present |
| **2. Results Overview** | ‚úÖ | ‚úÖ | Both present |
| **3. Key Findings** | ‚úÖ | ‚úÖ | Both present |
| **3.1 Path Completion** | ‚úÖ | ‚úÖ | Both present |
| **3.2 Trip Length Realism** | ‚úÖ | ‚úÖ | Both present |
| **3.3 Spatial Distribution** | ‚úÖ | ‚úÖ | Both present |
| **3.4 Generalization** | ‚úÖ | ‚úÖ | Both present |
| **3.5 Scenario-Level Analysis** | ‚úÖ | ‚úÖ | **Both added (Oct 31, 2025)** |
| **3.5.1 Per-Scenario Tables** | ‚úÖ | ‚úÖ | Identical structure |
| **3.5.2 Key Scenario Findings** | ‚úÖ | ‚úÖ | Dataset-specific insights |
| **3.5.3 Notable Scenarios** | ‚úÖ | ‚úÖ | Top-5 lists |
| **4. Dataset-Specific Section** | "Why Vanilla Fails" | "Porto vs Beijing" | Appropriate for each |
| **5. What Distillation Transferred** | ‚úÖ | ‚úÖ (Phase 1) | Both present |
| **6. Trajectory-Level Analysis** | ‚úÖ | ‚úÖ | Both present |
| **7. Statistical Summary** | ‚úÖ | ‚úÖ | Both present |
| **8. Conclusions** | ‚úÖ | ‚úÖ | Both present |
| **9. Appendix** | ‚úÖ | ‚úÖ | Both present |

**Total Lines:**
- Beijing: 882 lines
- Porto: 975 lines (includes Phase 2 context)

---

## Scenario Analysis Coverage

### Subsection Structure

| Subsection | Beijing | Porto | Status |
|------------|---------|-------|--------|
| **Scenario Taxonomy** | ‚úÖ 9 scenarios | ‚úÖ 9 scenarios | ‚úÖ **MATCHED** |
| **Scenario Distribution Plots** | ‚úÖ 2 figures | ‚úÖ 2 figures | ‚úÖ **MATCHED** |
| **Per-Scenario Table (Train)** | ‚ùå Not shown | ‚ùå Not shown | Both omit train (focus on test) |
| **Per-Scenario Table (Test)** | ‚úÖ 9 rows | ‚úÖ 9 rows | ‚úÖ **MATCHED** |
| **Metric Comparison Plots** | ‚úÖ 2 figures | ‚úÖ 2 figures | ‚úÖ **MATCHED** |
| **Hierarchical Breakdowns** | ‚úÖ 2 figures | ‚úÖ 2 figures | ‚úÖ **MATCHED** |
| **Scenario-Specific Insights** | ‚úÖ Detailed | ‚úÖ Detailed | ‚úÖ **MATCHED** |
| **Notable Scenarios (Top-5)** | ‚úÖ Lists | ‚úÖ Lists | ‚úÖ **MATCHED** |
| **Cross-Dataset Comparison** | ‚úÖ Brief | ‚úÖ Detailed | Porto has more context |

### Scenarios Analyzed (Both Datasets)

**Temporal:**
- `weekday` (70-71%)
- `weekend` (29-30%)
- `peak` (8-11%)
- `off_peak` (88-92%)

**Spatial:**
- `city_center` (88-91%)
- `suburban` (9-12%)
- `within_center` (60-62%)
- `to_center` (14-16%)
- `from_center` (10-17%)

**Status:** ‚úÖ **IDENTICAL TAXONOMY**

### Aggregated Analysis Outputs

| Output | Beijing | Porto | Status |
|--------|---------|-------|--------|
| **scenarios_train.csv** | ‚úÖ 9 scenarios | ‚úÖ 9 scenarios | ‚úÖ **MATCHED** |
| **scenarios_test.csv** | ‚úÖ 9 scenarios | ‚úÖ 9 scenarios | ‚úÖ **MATCHED** |
| **top_scenarios_train.csv** | ‚úÖ 30 entries | ‚úÖ 30 entries | ‚úÖ **MATCHED** |
| **top_scenarios_test.csv** | ‚úÖ 30 entries | ‚úÖ 30 entries | ‚úÖ **MATCHED** |
| **aggregates.json** | ‚úÖ Present | ‚úÖ Present | ‚úÖ **MATCHED** |
| **md/scenario_analysis.md** | ‚úÖ Generated | ‚úÖ Generated | ‚úÖ **MATCHED** |

---

## Key Findings Comparison

### Beijing Findings

1. **Distillation dramatically improves** (85-89% OD vs 12-18%)
2. **Vanilla catastrophically fails** across all scenarios
3. **Distance JSD reduced 87%** (0.145 ‚Üí 0.018)
4. **Radius JSD reduced 98%** (0.198 ‚Üí 0.003)
5. **Universal scenario benefits** (all Œî large and negative)
6. **Long-distance navigation** benefits most (Œî = -0.24)

### Porto Findings

1. **Both models perform well** (87-92% OD for both)
2. **Minimal distillation benefit** with Phase 1 hyperparameters
3. **Distance JSD similar** (distilled 0.006 vs vanilla 0.0055)
4. **Radius JSD similar** (distilled 0.011 vs vanilla 0.011)
5. **Scenario-dependent benefits** (¬±Œî mixed, average near-zero)
6. **Dense urban scenarios** show marginal distilled advantage

**Interpretation:** Task complexity determines distillation value.

---

## Missing Analyses

### ‚ùå Inference Speed / Computational Performance

**Neither document includes:**
- Trajectory generation time (seconds per trajectory)
- Throughput metrics (trajectories per second)
- Beam search timing breakdown
- Model inference latency
- GPU vs CPU performance comparison
- Memory usage during generation
- Batch generation efficiency

**What's present:**
- Beijing: "Caching for efficiency", "GPU for generation"
- Porto: "GPU-accelerated beam search", "CPU-based evaluation"

**Status:** ‚ö†Ô∏è **GAP IN BOTH DOCUMENTS**

**Impact:** Cannot assess:
- Whether distillation adds computational overhead
- Real-time generation feasibility
- Scalability to large-scale generation
- Hardware requirements for deployment

### ‚ùå Training Time / Convergence Analysis

**Neither document includes:**
- Training time per epoch
- Total training wall-clock time
- Convergence curves (train vs val loss over epochs)
- Early stopping analysis
- GPU utilization during training
- Memory footprint during training

**What's present:**
- Both: "25 epochs", training hyperparameters
- Neither: Actual timing or convergence behavior

**Status:** ‚ö†Ô∏è **GAP IN BOTH DOCUMENTS**

### ‚ùå Ablation Studies

**Neither document includes:**
- Effect of varying distillation hyperparameters (Œª, œÑ, w)
- Teacher vs student architecture comparison
- Alternative teacher models
- Distillation window size sensitivity

**What's present:**
- Porto: References to Phase 1 vs Phase 2 hyperparameters
- Beijing: References to Optuna tuning (but not detailed ablations)

**Status:** ‚ö†Ô∏è **GAP IN BOTH DOCUMENTS** (though Porto has more context via Hyperparameter-Optimization-Porto.md)

### ‚ùå Error Analysis / Failure Case Studies

**Neither document includes:**
- Specific failure case examples with trajectory visualizations
- Categorization of failure modes (stuck, loops, wrong direction)
- Spatial distribution of failures (where do models fail?)
- OD-pair difficulty analysis (which OD pairs are hardest?)

**What's present:**
- Beijing: General description of vanilla failures ("gets stuck", "stops early")
- Porto: Brief mention of vanilla success
- Both: Multi-scenario trajectory grids (but no detailed failure analysis)

**Status:** ‚ö†Ô∏è **PARTIAL COVERAGE** (qualitative descriptions, no systematic analysis)

### ‚ùå Model Size / Parameter Count Comparison

**Neither document includes:**
- Number of parameters (vanilla vs distilled)
- Model size on disk (MB)
- Architecture details (layer counts, hidden dimensions)

**What's present:**
- Both: "Identical architecture" for vanilla vs distilled
- Neither: Actual parameter counts or model sizes

**Status:** ‚ö†Ô∏è **GAP IN BOTH DOCUMENTS**

---

## Visualizations Comparison

| Visualization Type | Beijing | Porto | Status |
|--------------------|---------|-------|--------|
| **Distance Distribution** | ‚úÖ train/test | ‚úÖ train/test | ‚úÖ **MATCHED** |
| **Radius Distribution** | ‚úÖ train/test | ‚úÖ train/test | ‚úÖ **MATCHED** |
| **OD Matching Rates** | ‚úÖ Bar chart | ‚úÖ In table | Beijing has dedicated figure |
| **JSD Comparison** | ‚úÖ Figure | ‚úÖ In table | Beijing has dedicated figure |
| **Metrics Heatmap** | ‚úÖ Figure | ‚ùå Missing | Beijing more comprehensive |
| **Performance Radar** | ‚úÖ Figure | ‚ùå Missing | Beijing more comprehensive |
| **Scenario Distribution** | ‚úÖ 2 models | ‚úÖ 2 models | ‚úÖ **MATCHED** |
| **Metric Comparison** | ‚úÖ 2 models | ‚úÖ 2 models | ‚úÖ **MATCHED** |
| **Hierarchical Plots** | ‚úÖ 2 models | ‚úÖ 2 models | ‚úÖ **MATCHED** |
| **Multi-Scenario Grids** | ‚ùå Not referenced | ‚úÖ 3 featured + 4 listed | Porto more detailed |
| **Train vs Test** | ‚úÖ Figure | ‚úÖ In table | Beijing has dedicated figure |
| **Seed Robustness** | ‚úÖ Figure | ‚úÖ In table | Beijing has dedicated figure |

**Summary:**
- Beijing: More standalone figures (8 primary + 4 distributions = 12)
- Porto: More trajectory visualizations (7 multi-scenario grids)
- Both: Complete scenario analysis visualizations (6 files each)

---

## Methodology Details Comparison

| Detail | Beijing | Porto | Status |
|--------|---------|-------|--------|
| **OD Matching Algorithm** | ‚úÖ Code snippet | ‚úÖ Code snippet | ‚úÖ **MATCHED** |
| **JSD Calculation** | ‚úÖ Formula + code | ‚úÖ Formula only | Beijing more detailed |
| **Metrics Formulas** | ‚úÖ All metrics | ‚úÖ All metrics | ‚úÖ **MATCHED** |
| **Grid Resolution** | ‚úÖ 0.001¬∞ (~111m) | ‚úÖ 0.001¬∞ (~111m) | ‚úÖ **MATCHED** |
| **Beam Search Width** | ‚úÖ Width 4 | ‚úÖ Width 4 | ‚úÖ **MATCHED** |
| **EDR Threshold** | ‚úÖ 100m | ‚úÖ 100m | ‚úÖ **MATCHED** |
| **Evaluation Pipeline** | ‚úÖ Brief | ‚úÖ More detailed | Porto lists specific scripts |
| **Hardware** | ‚úÖ Generic | ‚úÖ Generic | Both lack specifics |
| **Reproducibility** | ‚úÖ Seed 42 | ‚úÖ Seeds 42/43/44 | Porto has more seeds |
| **Scenario Aggregation** | ‚úÖ Command | ‚úÖ Command | ‚úÖ **MATCHED** |

---

## Appendix Completeness

| Item | Beijing | Porto | Status |
|------|---------|-------|--------|
| **Figure List** | ‚úÖ 12 figures | ‚úÖ Comprehensive | Porto more detailed |
| **Scenario Assets** | ‚úÖ Added (Oct 31) | ‚úÖ Added (Oct 31) | ‚úÖ **MATCHED** |
| **Aggregation Script** | ‚úÖ Command | ‚úÖ Command | ‚úÖ **MATCHED** |
| **OD Matching Code** | ‚úÖ Python snippet | ‚úÖ Python snippet | ‚úÖ **MATCHED** |
| **JSD Calculation** | ‚úÖ Formula + bins | ‚úÖ Formula only | Beijing more detailed |
| **Data Sources** | ‚úÖ Counts | ‚úÖ Counts | ‚úÖ **MATCHED** |
| **Computational Details** | ‚úÖ Brief | ‚úÖ More detailed | Porto lists software |

---

## Recommendations

### Priority 1: Add Inference Speed Analysis (Both Documents)

Add a new subsection: **"6.X Inference Performance"** or **"9.X Computational Performance"**

**Metrics to include:**
- Generation time per trajectory (mean ¬± std)
- Throughput (trajectories/second)
- Beam search breakdown (time per step)
- Memory usage (GPU/CPU)
- Batch vs single trajectory efficiency
- Vanilla vs distilled comparison

**Data sources:**
- Profile `gene.py` with timing instrumentation
- Use `torch.cuda.Event()` for GPU timing
- Log memory with `torch.cuda.max_memory_allocated()`
- Measure on standardized hardware (specify GPU model)

### Priority 2: Harmonize Visualizations

**Beijing gaps:**
- Add multi-scenario trajectory grid references (if available)
- More detailed trajectory visualization examples

**Porto gaps:**
- Consider adding dedicated OD matching rate figure
- Consider adding performance radar chart
- Consider adding metrics heatmap

**Both:**
- Ensure all referenced figures exist and are accessible
- Use consistent naming conventions
- Include figure captions with interpretation

### Priority 3: Add Training Convergence Analysis

Add to Appendix or Section 5:
- Training loss curves (train vs val over 25 epochs)
- Wall-clock training time
- GPU utilization during training
- Memory footprint comparison

### Priority 4: Enhance Error Analysis

Add subsection under Section 6 or dedicated section:
- Failure case taxonomy (categorize by type)
- Spatial distribution of failures (heat map)
- Difficult OD pair analysis (which pairs fail most?)
- Trajectory visualizations of specific failures

### Priority 5: Document Model Specifications

Add to Section 1 or Appendix:
- Parameter count (total, by layer)
- Model size on disk
- Architecture diagram (if not already in LMTAD-Distillation.md)
- Inference memory requirements

---

## Summary

### ‚úÖ Well-Matched Aspects

1. **Scenario-level analysis**: Both documents now have comprehensive, identically-structured scenario analyses
2. **Core evaluation metrics**: All key metrics (JSD, OD coverage, DTW, etc.) present in both
3. **Methodology**: Grid resolution, beam width, evaluation protocol all consistent
4. **Aggregation tooling**: Both use the same reusable script with identical outputs

### ‚ö†Ô∏è Gaps Present in BOTH Documents

1. **Inference speed**: No timing, throughput, or latency analysis
2. **Training performance**: No convergence curves or training time data
3. **Ablation studies**: Limited analysis of hyperparameter sensitivity
4. **Systematic error analysis**: No categorization or spatial analysis of failures
5. **Model specifications**: No parameter counts or size information

### üìä Dataset-Specific Differences (Appropriate)

1. **Beijing "Why Vanilla Fails"** vs **Porto "Porto vs Beijing"** - makes sense given findings
2. **Beijing has more standalone figures** (heatmaps, radar charts) - appropriate for dramatic differences
3. **Porto has more trajectory grids** - appropriate for nuanced comparisons
4. **Porto includes Phase 2 context** - dataset-specific phased approach

### üéØ Action Items

1. **Add inference speed analysis** to both documents (Priority 1)
2. **Harmonize visualization coverage** (Priority 2)
3. **Document training convergence** (Priority 3)
4. **Enhance error analysis** (Priority 4)
5. **Add model specifications** (Priority 5)

---

**Generated:** October 31, 2025  
**Comparison Version:** 1.0  
**Documents Compared:**
- Beijing: `/home/matt/Dev/HOSER/hoser-distill-optuna-6/EVALUATION_ANALYSIS.md` (882 lines)
- Porto: `/home/matt/Dev/HOSER/hoser-distill-optuna-porto-eval-eb0e88ab-20251026_152732/EVALUATION_ANALYSIS_PHASE1.md` (975 lines)

---

## ‚úÖ Search Method Ablation Study (November 2025)

**Status:** **COMPLETED** - Addresses computational performance and search method dependency gaps

**Study:** Beijing dataset beam search ablation (Issue #8)  
**Date:** November 5-6, 2025  
**Computation:** 26 hours (12 trajectory generation runs + 12 evaluation runs)

### Motivation

**Research Question:** Are distillation benefits dependent on search method (A* vs beam search)?

**Why this matters:**
- Original HOSER paper uses A* search (greedy heuristic-guided)
- Current implementation defaults to beam search (width=4)
- Cannot separate model quality improvements from search effectiveness without ablation

### Experimental Design

**Models Tested:**
- `distilled` (seed 42)
- `distilled_seed44` (independent seed)
- `vanilla` (baseline)

**Search Methods:**
1. **A* Search** - Original HOSER greedy algorithm (single-path exploration)
2. **Beam Search (width=4)** - Parallel exploration (current default)

**Configuration:**
- Dataset: Beijing
- Trajectories: 1,000 per model/OD-source/search-method
- Seed: 42 (reproducible)
- OD Sources: train, test
- Metrics: Speed (traj/s), forward passes, DTW_norm, Hausdorff_norm, EDR

**Total Runs:** 24 evaluations (3 models √ó 2 OD sources √ó 2 search methods)

### Key Findings: Search-Method Interaction Effect

#### üî¨ Performance Results

| Search Method | Distilled Speed | Vanilla Speed | Speedup Ratio |
|---------------|-----------------|---------------|---------------|
| **A* Search** | 0.30 traj/s | 0.05 traj/s | **6.0x faster** ‚¨ÜÔ∏è |
| **Beam Search (width=4)** | 1.79 traj/s | 2.46 traj/s | **0.73x (1.4x slower!)** ‚¨áÔ∏è |

**Critical Finding:** Distillation benefit **flips** depending on search method!

#### Forward Passes per Trajectory

| Search Method | Distilled | Vanilla | Exploration Efficiency |
|---------------|-----------|---------|------------------------|
| **A* Search** | ~500 passes | ~3100 passes | **6.2x fewer** ‚¨ÜÔ∏è |
| **Beam Search (width=4)** | ~43 passes | ~32 passes | **1.3x more** ‚¨áÔ∏è |

**Interpretation:**
- **A* benefits from distillation**: Better heuristic guidance ‚Üí fewer backtracks ‚Üí faster
- **Beam negates distillation advantage**: Parallel exploration reduces dependency on prediction quality

#### Trajectory Quality (Normalized Metrics)

**A* Search:**

| Model | DTW_norm (km/point) | Hausdorff_norm (km/point) | EDR |
|-------|---------------------|---------------------------|-----|
| Distilled (test) | 0.363 | 0.023 | 0.439 |
| Vanilla (test) | 0.347 | 0.029 | 0.483 |

**Beam Search:**

| Model | DTW_norm (km/point) | Hausdorff_norm (km/point) | EDR |
|-------|---------------------|---------------------------|-----|
| Distilled (test) | 0.559 | 0.025 | 0.492 |
| Vanilla (test) | 0.358 | 0.033 | 0.528 |

**Pattern:** Vanilla shows lower DTW_norm (better local trajectory quality) across both search methods

#### OD Destination Matching

| Model + Search | OD Match Rate |
|----------------|---------------|
| Distilled + A* | ~98% |
| Distilled + Beam | 85-86% |
| Vanilla + A* | 19% |
| Vanilla + Beam | 19% |

**Key Insight:** Distillation dramatically improves OD matching (4-5x better) regardless of search method

### Interpretation: Context-Dependent Benefits

#### ‚úÖ Distillation Improves:
1. **OD Destination Matching** - 85-98% vs 19% (4-5x improvement)
2. **Realistic Trajectory Lengths** - Distilled generates appropriate-length paths
3. **A* Search Speed** - 6x faster with better heuristic guidance

#### ‚ùå Distillation Does NOT Improve:
1. **Beam Search Speed** - 1.4x slower with distilled model
2. **Local Trajectory Quality** - Vanilla has lower DTW_norm per point
3. **Universal Search Benefits** - Effect reverses depending on search method

#### üéØ Recommendation Matrix

| Use Case | Recommended Configuration | Rationale |
|----------|--------------------------|-----------|
| **Real-time generation (speed priority)** | Vanilla + Beam (width=4) | Fastest (2.46 traj/s) |
| **High OD matching (quality priority)** | Distilled + Beam (width=4) | 85-86% OD match, realistic lengths |
| **Distilled model deployment** | Distilled + A* | Leverages distillation advantage (6x speedup) |
| **Trajectory diversity** | Any + Beam | Parallel exploration provides variety |

### Research Implications

**Strengthens Contribution:**
- ‚úÖ Shows distillation has clear, measurable benefits (OD matching)
- ‚úÖ Provides nuanced analysis (not universal improvement)
- ‚úÖ Uses trajectory-length independent metrics (fair comparison)
- ‚úÖ Addresses search method dependence concerns

**Honest Limitations:**
- ‚ö†Ô∏è Distillation benefit is context-dependent
- ‚ö†Ô∏è Search method choice significantly impacts performance
- ‚ö†Ô∏è Speed-quality tradeoffs must be considered for deployment

**Publication Value:**
- Demonstrates thorough experimental validation
- Shows understanding of method limitations
- Provides actionable deployment guidance
- Uses rigorous statistical methodology

### Cross-Dataset Validation

**Beijing Study:** Completed (documented here)

**Porto Study:** Tracked in Issue #44 (optional future work)
- Different baseline: Porto vanilla succeeds (88% OD match) vs Beijing vanilla fails (19%)
- Tests whether interaction persists when vanilla performs well
- Validates with Phase 2 optimized hyperparameters

**Decision:** Beijing ablation sufficient for publication, Porto validation strengthens generalizability claims

### Files Generated

**Trajectory Data:** (47MB total)
- 12 CSV files: `hoser-distill-optuna-6/gene/Beijing/seed42/*.csv`
- 12 performance JSON: `*_perf.json` with timing metrics

**Evaluation Results:**
- 12 evaluation directories with normalized metrics
- `results.json` with DTW_norm, Hausdorff_norm, EDR

**Scripts:**
- `run_beam_ablation.sh` - Automated ablation execution
- `complete_beam_ablation_with_normalized_metrics.sh` - Full study with normalization

### Integration with Other Improvements

**Synchronized with Issue #14 (Normalized Metrics):**
- Ablation uses trajectory-length independent metrics throughout
- Fair comparison between A* (shorter paths) and Beam (longer paths)
- Enables scientific rigor in search method comparison

**Complements Issue #16 (Paired Statistical Tests):**
- Provides performance data for future paired comparisons
- Documents variance sources (search method + model)
- Supports multi-factor statistical analysis

### Conclusion

**Resolution:** ‚úÖ **Can now separate model quality from search effectiveness**

**Finding:** Distillation benefit is **search-method dependent**:
- A* leverages distillation for 6x speedup
- Beam search shows opposite pattern (vanilla 1.4x faster)
- OD matching improves with distillation regardless of search method

**Impact:** Provides deployment guidance based on use case priorities (speed vs OD matching vs trajectory quality)

