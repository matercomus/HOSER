# Cross-Dataset Evaluation Analysis Comparison

**Date:** October 31, 2025  
**Purpose:** Compare Beijing and Porto evaluation analyses for completeness and consistency

---

## Document Structure Comparison

| Section | Beijing | Porto | Notes |
|---------|---------|-------|-------|
| **Executive Summary** | âœ… | âœ… | Both present |
| **1. Experimental Setup** | âœ… | âœ… | Both present |
| **2. Results Overview** | âœ… | âœ… | Both present |
| **3. Key Findings** | âœ… | âœ… | Both present |
| **3.1 Path Completion** | âœ… | âœ… | Both present |
| **3.2 Trip Length Realism** | âœ… | âœ… | Both present |
| **3.3 Spatial Distribution** | âœ… | âœ… | Both present |
| **3.4 Generalization** | âœ… | âœ… | Both present |
| **3.5 Scenario-Level Analysis** | âœ… | âœ… | **Both added (Oct 31, 2025)** |
| **3.5.1 Per-Scenario Tables** | âœ… | âœ… | Identical structure |
| **3.5.2 Key Scenario Findings** | âœ… | âœ… | Dataset-specific insights |
| **3.5.3 Notable Scenarios** | âœ… | âœ… | Top-5 lists |
| **4. Dataset-Specific Section** | "Why Vanilla Fails" | "Porto vs Beijing" | Appropriate for each |
| **5. What Distillation Transferred** | âœ… | âœ… (Phase 1) | Both present |
| **6. Trajectory-Level Analysis** | âœ… | âœ… | Both present |
| **7. Statistical Summary** | âœ… | âœ… | Both present |
| **8. Conclusions** | âœ… | âœ… | Both present |
| **9. Appendix** | âœ… | âœ… | Both present |

**Total Lines:**
- Beijing: 882 lines
- Porto: 975 lines (includes Phase 2 context)

---

## Scenario Analysis Coverage

### Subsection Structure

| Subsection | Beijing | Porto | Status |
|------------|---------|-------|--------|
| **Scenario Taxonomy** | âœ… 9 scenarios | âœ… 9 scenarios | âœ… **MATCHED** |
| **Scenario Distribution Plots** | âœ… 2 figures | âœ… 2 figures | âœ… **MATCHED** |
| **Per-Scenario Table (Train)** | âŒ Not shown | âŒ Not shown | Both omit train (focus on test) |
| **Per-Scenario Table (Test)** | âœ… 9 rows | âœ… 9 rows | âœ… **MATCHED** |
| **Metric Comparison Plots** | âœ… 2 figures | âœ… 2 figures | âœ… **MATCHED** |
| **Hierarchical Breakdowns** | âœ… 2 figures | âœ… 2 figures | âœ… **MATCHED** |
| **Scenario-Specific Insights** | âœ… Detailed | âœ… Detailed | âœ… **MATCHED** |
| **Notable Scenarios (Top-5)** | âœ… Lists | âœ… Lists | âœ… **MATCHED** |
| **Cross-Dataset Comparison** | âœ… Brief | âœ… Detailed | Porto has more context |

### Scenarios Analyzed (Both Datasets)

**Temporal:**
- `weekday` (70-71%)
- `weekend` (29-30%)
- `peak` (8-11%)
- `off_peak` (88-92%)

**Spatial:**
- `city_center` (88-91%)
- `suburban` (9-12%)
- `within_center` (60-62%)
- `to_center` (14-16%)
- `from_center` (10-17%)

**Status:** âœ… **IDENTICAL TAXONOMY**

### Aggregated Analysis Outputs

| Output | Beijing | Porto | Status |
|--------|---------|-------|--------|
| **scenarios_train.csv** | âœ… 9 scenarios | âœ… 9 scenarios | âœ… **MATCHED** |
| **scenarios_test.csv** | âœ… 9 scenarios | âœ… 9 scenarios | âœ… **MATCHED** |
| **top_scenarios_train.csv** | âœ… 30 entries | âœ… 30 entries | âœ… **MATCHED** |
| **top_scenarios_test.csv** | âœ… 30 entries | âœ… 30 entries | âœ… **MATCHED** |
| **aggregates.json** | âœ… Present | âœ… Present | âœ… **MATCHED** |
| **md/scenario_analysis.md** | âœ… Generated | âœ… Generated | âœ… **MATCHED** |

---

## Key Findings Comparison

### Beijing Findings

1. **Distillation dramatically improves** (85-89% OD vs 12-18%)
2. **Vanilla catastrophically fails** across all scenarios
3. **Distance JSD reduced 87%** (0.145 â†’ 0.018)
4. **Radius JSD reduced 98%** (0.198 â†’ 0.003)
5. **Universal scenario benefits** (all Î” large and negative)
6. **Long-distance navigation** benefits most (Î” = -0.24)

### Porto Findings

1. **Both models perform well** (87-92% OD for both)
2. **Minimal distillation benefit** with Phase 1 hyperparameters
3. **Distance JSD similar** (distilled 0.006 vs vanilla 0.0055)
4. **Radius JSD similar** (distilled 0.011 vs vanilla 0.011)
5. **Scenario-dependent benefits** (Â±Î” mixed, average near-zero)
6. **Dense urban scenarios** show marginal distilled advantage

**Interpretation:** Task complexity determines distillation value.

---

## Missing Analyses

### âŒ Inference Speed / Computational Performance

**Neither document includes:**
- Trajectory generation time (seconds per trajectory)
- Throughput metrics (trajectories per second)
- Beam search timing breakdown
- Model inference latency
- GPU vs CPU performance comparison
- Memory usage during generation
- Batch generation efficiency

**What's present:**
- Beijing: "Caching for efficiency", "GPU for generation"
- Porto: "GPU-accelerated beam search", "CPU-based evaluation"

**Status:** âš ï¸ **GAP IN BOTH DOCUMENTS**

**Impact:** Cannot assess:
- Whether distillation adds computational overhead
- Real-time generation feasibility
- Scalability to large-scale generation
- Hardware requirements for deployment

### âŒ Training Time / Convergence Analysis

**Neither document includes:**
- Training time per epoch
- Total training wall-clock time
- Convergence curves (train vs val loss over epochs)
- Early stopping analysis
- GPU utilization during training
- Memory footprint during training

**What's present:**
- Both: "25 epochs", training hyperparameters
- Neither: Actual timing or convergence behavior

**Status:** âš ï¸ **GAP IN BOTH DOCUMENTS**

### âŒ Ablation Studies

**Neither document includes:**
- Effect of varying distillation hyperparameters (Î», Ï„, w)
- Teacher vs student architecture comparison
- Alternative teacher models
- Distillation window size sensitivity

**What's present:**
- Porto: References to Phase 1 vs Phase 2 hyperparameters
- Beijing: References to Optuna tuning (but not detailed ablations)

**Status:** âš ï¸ **GAP IN BOTH DOCUMENTS** (though Porto has more context via Hyperparameter-Optimization-Porto.md)

### âŒ Error Analysis / Failure Case Studies

**Neither document includes:**
- Specific failure case examples with trajectory visualizations
- Categorization of failure modes (stuck, loops, wrong direction)
- Spatial distribution of failures (where do models fail?)
- OD-pair difficulty analysis (which OD pairs are hardest?)

**What's present:**
- Beijing: General description of vanilla failures ("gets stuck", "stops early")
- Porto: Brief mention of vanilla success
- Both: Multi-scenario trajectory grids (but no detailed failure analysis)

**Status:** âš ï¸ **PARTIAL COVERAGE** (qualitative descriptions, no systematic analysis)

### âŒ Model Size / Parameter Count Comparison

**Neither document includes:**
- Number of parameters (vanilla vs distilled)
- Model size on disk (MB)
- Architecture details (layer counts, hidden dimensions)

**What's present:**
- Both: "Identical architecture" for vanilla vs distilled
- Neither: Actual parameter counts or model sizes

**Status:** âš ï¸ **GAP IN BOTH DOCUMENTS**

---

## Visualizations Comparison

| Visualization Type | Beijing | Porto | Status |
|--------------------|---------|-------|--------|
| **Distance Distribution** | âœ… train/test | âœ… train/test | âœ… **MATCHED** |
| **Radius Distribution** | âœ… train/test | âœ… train/test | âœ… **MATCHED** |
| **OD Matching Rates** | âœ… Bar chart | âœ… In table | Beijing has dedicated figure |
| **JSD Comparison** | âœ… Figure | âœ… In table | Beijing has dedicated figure |
| **Metrics Heatmap** | âœ… Figure | âŒ Missing | Beijing more comprehensive |
| **Performance Radar** | âœ… Figure | âŒ Missing | Beijing more comprehensive |
| **Scenario Distribution** | âœ… 2 models | âœ… 2 models | âœ… **MATCHED** |
| **Metric Comparison** | âœ… 2 models | âœ… 2 models | âœ… **MATCHED** |
| **Hierarchical Plots** | âœ… 2 models | âœ… 2 models | âœ… **MATCHED** |
| **Multi-Scenario Grids** | âŒ Not referenced | âœ… 3 featured + 4 listed | Porto more detailed |
| **Train vs Test** | âœ… Figure | âœ… In table | Beijing has dedicated figure |
| **Seed Robustness** | âœ… Figure | âœ… In table | Beijing has dedicated figure |

**Summary:**
- Beijing: More standalone figures (8 primary + 4 distributions = 12)
- Porto: More trajectory visualizations (7 multi-scenario grids)
- Both: Complete scenario analysis visualizations (6 files each)

---

## Methodology Details Comparison

| Detail | Beijing | Porto | Status |
|--------|---------|-------|--------|
| **OD Matching Algorithm** | âœ… Code snippet | âœ… Code snippet | âœ… **MATCHED** |
| **JSD Calculation** | âœ… Formula + code | âœ… Formula only | Beijing more detailed |
| **Metrics Formulas** | âœ… All metrics | âœ… All metrics | âœ… **MATCHED** |
| **Grid Resolution** | âœ… 0.001Â° (~111m) | âœ… 0.001Â° (~111m) | âœ… **MATCHED** |
| **Beam Search Width** | âœ… Width 4 | âœ… Width 4 | âœ… **MATCHED** |
| **EDR Threshold** | âœ… 100m | âœ… 100m | âœ… **MATCHED** |
| **Evaluation Pipeline** | âœ… Brief | âœ… More detailed | Porto lists specific scripts |
| **Hardware** | âœ… Generic | âœ… Generic | Both lack specifics |
| **Reproducibility** | âœ… Seed 42 | âœ… Seeds 42/43/44 | Porto has more seeds |
| **Scenario Aggregation** | âœ… Command | âœ… Command | âœ… **MATCHED** |

---

## Appendix Completeness

| Item | Beijing | Porto | Status |
|------|---------|-------|--------|
| **Figure List** | âœ… 12 figures | âœ… Comprehensive | Porto more detailed |
| **Scenario Assets** | âœ… Added (Oct 31) | âœ… Added (Oct 31) | âœ… **MATCHED** |
| **Aggregation Script** | âœ… Command | âœ… Command | âœ… **MATCHED** |
| **OD Matching Code** | âœ… Python snippet | âœ… Python snippet | âœ… **MATCHED** |
| **JSD Calculation** | âœ… Formula + bins | âœ… Formula only | Beijing more detailed |
| **Data Sources** | âœ… Counts | âœ… Counts | âœ… **MATCHED** |
| **Computational Details** | âœ… Brief | âœ… More detailed | Porto lists software |

---

## Recommendations

### Priority 1: Add Inference Speed Analysis (Both Documents)

Add a new subsection: **"6.X Inference Performance"** or **"9.X Computational Performance"**

**Metrics to include:**
- Generation time per trajectory (mean Â± std)
- Throughput (trajectories/second)
- Beam search breakdown (time per step)
- Memory usage (GPU/CPU)
- Batch vs single trajectory efficiency
- Vanilla vs distilled comparison

**Data sources:**
- Profile `gene.py` with timing instrumentation
- Use `torch.cuda.Event()` for GPU timing
- Log memory with `torch.cuda.max_memory_allocated()`
- Measure on standardized hardware (specify GPU model)

### Priority 2: Harmonize Visualizations

**Beijing gaps:**
- Add multi-scenario trajectory grid references (if available)
- More detailed trajectory visualization examples

**Porto gaps:**
- Consider adding dedicated OD matching rate figure
- Consider adding performance radar chart
- Consider adding metrics heatmap

**Both:**
- Ensure all referenced figures exist and are accessible
- Use consistent naming conventions
- Include figure captions with interpretation

### Priority 3: Add Training Convergence Analysis

Add to Appendix or Section 5:
- Training loss curves (train vs val over 25 epochs)
- Wall-clock training time
- GPU utilization during training
- Memory footprint comparison

### Priority 4: Enhance Error Analysis

Add subsection under Section 6 or dedicated section:
- Failure case taxonomy (categorize by type)
- Spatial distribution of failures (heat map)
- Difficult OD pair analysis (which pairs fail most?)
- Trajectory visualizations of specific failures

### Priority 5: Document Model Specifications

Add to Section 1 or Appendix:
- Parameter count (total, by layer)
- Model size on disk
- Architecture diagram (if not already in LMTAD-Distillation.md)
- Inference memory requirements

---

## Summary

### âœ… Well-Matched Aspects

1. **Scenario-level analysis**: Both documents now have comprehensive, identically-structured scenario analyses
2. **Core evaluation metrics**: All key metrics (JSD, OD coverage, DTW, etc.) present in both
3. **Methodology**: Grid resolution, beam width, evaluation protocol all consistent
4. **Aggregation tooling**: Both use the same reusable script with identical outputs

### âš ï¸ Gaps Present in BOTH Documents

1. **Inference speed**: No timing, throughput, or latency analysis
2. **Training performance**: No convergence curves or training time data
3. **Ablation studies**: Limited analysis of hyperparameter sensitivity
4. **Systematic error analysis**: No categorization or spatial analysis of failures
5. **Model specifications**: No parameter counts or size information

### ğŸ“Š Dataset-Specific Differences (Appropriate)

1. **Beijing "Why Vanilla Fails"** vs **Porto "Porto vs Beijing"** - makes sense given findings
2. **Beijing has more standalone figures** (heatmaps, radar charts) - appropriate for dramatic differences
3. **Porto has more trajectory grids** - appropriate for nuanced comparisons
4. **Porto includes Phase 2 context** - dataset-specific phased approach

### ğŸ¯ Action Items

1. **Add inference speed analysis** to both documents (Priority 1)
2. **Harmonize visualization coverage** (Priority 2)
3. **Document training convergence** (Priority 3)
4. **Enhance error analysis** (Priority 4)
5. **Add model specifications** (Priority 5)

---

**Generated:** October 31, 2025  
**Comparison Version:** 1.0  
**Documents Compared:**
- Beijing: `/home/matt/Dev/HOSER/hoser-distill-optuna-6/EVALUATION_ANALYSIS.md` (882 lines)
- Porto: `/home/matt/Dev/HOSER/hoser-distill-optuna-porto-eval-eb0e88ab-20251026_152732/EVALUATION_ANALYSIS_PHASE1.md` (975 lines)

